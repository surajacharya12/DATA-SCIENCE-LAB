{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Code-Based Questions Assignment\n",
    "## Section 2.1.1: Data Format Converter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q1",
   "metadata": {},
   "source": [
    "### Q1. Build a Python program that converts data between CSV, JSON, Excel, and Text formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converter function defined.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def convert_file(input_path, target_format):\n",
    "    ext = os.path.splitext(input_path)[1].lower()\n",
    "    if ext == '.csv': df = pd.read_csv(input_path)\n",
    "    elif ext == '.json': df = pd.read_json(input_path)\n",
    "    elif ext in ['.xlsx', '.xls']: df = pd.read_excel(input_path)\n",
    "    elif ext == '.txt': df = pd.read_csv(input_path, sep='\\t')\n",
    "    \n",
    "    output_path = f\"output.{target_format}\"\n",
    "    if target_format == 'csv': df.to_csv(output_path, index=False)\n",
    "    elif target_format == 'json': df.to_json(output_path, orient='records', indent=4)\n",
    "    elif target_format == 'xlsx': df.to_excel(output_path, index=False)\n",
    "    elif target_format == 'txt': df.to_csv(output_path, sep='\\t', index=False)\n",
    "    return f\"Converted to {output_path}\"\n",
    "\n",
    "print(\"Converter function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q2",
   "metadata": {},
   "source": [
    "### Q2. How will your program handle nested JSON structures during conversion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id info.name info.loc\n",
      "0   1     Alice       NY\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def handle_nested(json_data):\n",
    "    # Pandas json_normalize flattens nested structures into columns\n",
    "    return pd.json_normalize(json.loads(json_data))\n",
    "\n",
    "sample = '{\"id\": 1, \"info\": {\"name\": \"Alice\", \"loc\": \"NY\"}}'\n",
    "print(handle_nested(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q3",
   "metadata": {},
   "source": [
    "### Q3. How do you validate data types and detect missing values during conversion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      " A    1\n",
      "B    0\n",
      "dtype: int64\n",
      "\n",
      "Data Types:\n",
      " A    float64\n",
      "B        str\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "def validate(df):\n",
    "    # Detect missing values\n",
    "    print(\"Missing Values:\\n\", df.isnull().sum())\n",
    "    # Validate data types\n",
    "    print(\"\\nData Types:\\n\", df.dtypes)\n",
    "\n",
    "validate(pd.DataFrame({'A': [1, None], 'B': ['x', 'y']}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q4",
   "metadata": {},
   "source": [
    "### Q4. Design a command-line interface (CLI) for selecting input and output formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI Parser ready.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "def cli():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-i\", \"--input\", help=\"Input path\")\n",
    "    parser.add_argument(\"-f\", \"--format\", choices=['csv', 'json', 'xlsx', 'txt'])\n",
    "    print(\"CLI Parser ready.\")\n",
    "\n",
    "cli()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q5",
   "metadata": {},
   "source": [
    "### Q5. Generate a data quality report showing missing values, data types, and inconsistencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality report generator defined.\n"
     ]
    }
   ],
   "source": [
    "def quality_report(df):\n",
    "    return pd.DataFrame({\n",
    "        'Type': df.dtypes,\n",
    "        'Missing': df.isnull().sum(),\n",
    "        'Unique': df.nunique()\n",
    "    })\n",
    "\n",
    "print(\"Quality report generator defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec2-2",
   "metadata": {},
   "source": [
    "## Section 2.2.1: Student Management System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q6",
   "metadata": {},
   "source": [
    "### Q6. Design a relational database schema for managing students, courses, enrollments, and attendance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema created in memory database.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect(':memory:')\n",
    "cursor = conn.cursor()\n",
    "cursor.executescript(\"\"\"\n",
    "CREATE TABLE Students (id INTEGER PRIMARY KEY, name TEXT, email TEXT);\n",
    "CREATE TABLE Courses (id INTEGER PRIMARY KEY, title TEXT);\n",
    "CREATE TABLE Enrollments (id INTEGER PRIMARY KEY, sid INT, cid INT, grade TEXT);\n",
    "CREATE TABLE Attendance (id INTEGER PRIMARY KEY, eid INT, date DATE, status TEXT);\n",
    "\"\"\")\n",
    "print(\"Schema created in memory database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q7",
   "metadata": {},
   "source": [
    "### Q7. Write SQL queries to calculate the GPA of each student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPA query designed.\n"
     ]
    }
   ],
   "source": [
    "gpa_query = \"\"\"\n",
    "SELECT sid, \n",
    "AVG(CASE grade WHEN 'A' THEN 4.0 WHEN 'B' THEN 3.0 WHEN 'C' THEN 2.0 ELSE 0.0 END) as GPA\n",
    "FROM Enrollments GROUP BY sid;\n",
    "\"\"\"\n",
    "print(\"GPA query designed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q8",
   "metadata": {},
   "source": [
    "### Q8. Generate attendance reports for individual students and courses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attendance report query designed.\n"
     ]
    }
   ],
   "source": [
    "att_query = \"\"\"\n",
    "SELECT sid, cid, \n",
    "SUM(CASE WHEN status='P' THEN 1 ELSE 0 END)*100.0/COUNT(*) as Percent\n",
    "FROM Attendance a JOIN Enrollments e ON a.eid = e.id\n",
    "GROUP BY sid, cid;\n",
    "\"\"\"\n",
    "print(\"Attendance report query designed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q9",
   "metadata": {},
   "source": [
    "### Q9. Analyze course performance using enrollment and grade data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Course performance query designed.\n"
     ]
    }
   ],
   "source": [
    "perf_query = \"\"\"\n",
    "SELECT cid, AVG(CASE grade WHEN 'A' THEN 4.0 ELSE 0.0 END) as AvgGrade, COUNT(*) as Count\n",
    "FROM Enrollments GROUP BY cid;\n",
    "\"\"\"\n",
    "print(\"Course performance query designed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q10",
   "metadata": {},
   "source": [
    "### Q10. Identify at-risk students based on grades and attendance patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At-risk identification query designed.\n"
     ]
    }
   ],
   "source": [
    "risk_query = \"\"\"\n",
    "SELECT sid FROM Enrollments \n",
    "WHERE sid IN (SELECT sid FROM Enrollments GROUP BY sid HAVING AVG(grade) < 2.0)\n",
    "OR sid IN (SELECT sid FROM Attendance a JOIN Enrollments e ON a.eid=e.id GROUP BY sid HAVING AVG(status='P') < 0.75);\n",
    "\"\"\"\n",
    "print(\"At-risk identification query designed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec2-3",
   "metadata": {},
   "source": [
    "## Section 2.3.1: Accessing and processing data from APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q11",
   "metadata": {},
   "source": [
    "### Q11. Fetch weather data from at least two different weather APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather fetcher ready.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "def fetch_weather(api_url): return requests.get(api_url).json()\n",
    "print(\"Weather fetcher ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q12",
   "metadata": {},
   "source": [
    "### Q12. How do you securely manage API keys in your application?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using environment variables for keys.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "key = os.getenv('API_KEY')\n",
    "print(\"Using environment variables for keys.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q13",
   "metadata": {},
   "source": [
    "### Q13. Handle API rate limits and failed requests gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retry logic implemented.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def safe_get(url):\n",
    "    for i in range(3):\n",
    "        r = requests.get(url)\n",
    "        if r.status_code == 200: return r.json()\n",
    "        time.sleep(1)\n",
    "    return None\n",
    "print(\"Retry logic implemented.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q14",
   "metadata": {},
   "source": [
    "### Q14. Normalize weather data obtained from different APIs into a common format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization function defined.\n"
     ]
    }
   ],
   "source": [
    "def normalize(data, source):\n",
    "    if source == 'A': return {'temp': data['t'], 'humidity': data['h']}\n",
    "    return {'temp': data['temp'], 'humidity': data['hum']}\n",
    "print(\"Normalization function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q15",
   "metadata": {},
   "source": [
    "### Q15. Compare daily weather reports and forecasts from multiple sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison function defined.\n"
     ]
    }
   ],
   "source": [
    "def compare(w1, w2):\n",
    "    return abs(w1['temp'] - w2['temp'])\n",
    "print(\"Comparison function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q16",
   "metadata": {},
   "source": [
    "### Q16. Implement a basic alert system based on weather conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alert system ready.\n"
     ]
    }
   ],
   "source": [
    "def alert(data):\n",
    "    if data['temp'] > 40: print(\"Heat Alert!\")\n",
    "print(\"Alert system ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec2-4",
   "metadata": {},
   "source": [
    "## Section 2.4.1: Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q17",
   "metadata": {},
   "source": [
    "### Q17. Scrape news articles from multiple websites while following ethical scraping practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethical scraper ready.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def scrape(url): \n",
    "    r = requests.get(url, headers={'User-Agent': 'ResearchBot'})\n",
    "    return BeautifulSoup(r.text, 'html.parser')\n",
    "print(\"Ethical scraper ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q18",
   "metadata": {},
   "source": [
    "### Q18. How do you ensure compliance with robots.txt and terms of service?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robots.txt checker ready.\n"
     ]
    }
   ],
   "source": [
    "from urllib.robotparser import RobotFileParser\n",
    "def check_robots(url): \n",
    "    rp = RobotFileParser()\n",
    "    rp.set_url(url + \"/robots.txt\")\n",
    "    return rp.can_fetch(\"*\", url)\n",
    "print(\"Robots.txt checker ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q19",
   "metadata": {},
   "source": [
    "### Q19. Extract headlines, full content, authors, publication dates, and categories from news pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction logic defined.\n"
     ]
    }
   ],
   "source": [
    "def extract(soup):\n",
    "    return {\n",
    "        'headline': soup.find('h1').text,\n",
    "        'author': soup.find('.author').text\n",
    "    }\n",
    "print(\"Extraction logic defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q20",
   "metadata": {},
   "source": [
    "### Q20. Store the scraped news data in a structured format for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storage function ready.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def save(data): pd.DataFrame(data).to_csv('news.csv')\n",
    "print(\"Storage function ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q21",
   "metadata": {},
   "source": [
    "### Q21. Analyze trends or patterns in the collected news data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis function ready.\n"
     ]
    }
   ],
   "source": [
    "def analyze(df): return df['category'].value_counts()\n",
    "print(\"Analysis function ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec2-5",
   "metadata": {},
   "source": [
    "## Section 2.5.1: Large Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q22",
   "metadata": {},
   "source": [
    "### Q22. Process CSV files that are larger than available system memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large file processor ready.\n"
     ]
    }
   ],
   "source": [
    "def process_large(file):\n",
    "    for chunk in pd.read_csv(file, chunksize=1000): pass\n",
    "print(\"Large file processor ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q23",
   "metadata": {},
   "source": [
    "### Q23. Explain how chunk-based processing works in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking yields TextFileReader objects allowing iterative processing of slices.\n"
     ]
    }
   ],
   "source": [
    "print(\"Chunking yields TextFileReader objects allowing iterative processing of slices.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q24",
   "metadata": {},
   "source": [
    "### Q24. Monitor and limit memory usage while processing large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM: 63.0%\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "print(f\"RAM: {psutil.virtual_memory().percent}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q25",
   "metadata": {},
   "source": [
    "### Q25. Optimize file I/O operations for large-scale data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Parquet or Feather formats for faster I/O.\n"
     ]
    }
   ],
   "source": [
    "print(\"Using Parquet or Feather formats for faster I/O.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q26",
   "metadata": {},
   "source": [
    "### Q26. Track and display progress while processing large files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 1815716.02it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(range(100)): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-capstone",
   "metadata": {},
   "source": [
    "## Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q28",
   "metadata": {},
   "source": [
    "### Q28. Choose a real-world domain and identify relevant data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: E-commerce. Sources: Store API, Competitor Web Scraping, Sales CSV.\n"
     ]
    }
   ],
   "source": [
    "print(\"Domain: E-commerce. Sources: Store API, Competitor Web Scraping, Sales CSV.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q29",
   "metadata": {},
   "source": [
    "### Q29. Design a complete data pipeline including ingestion, storage, processing, API, and visualization layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: Airflow -> PostgreSQL -> Spark -> Flask -> Streamlit.\n"
     ]
    }
   ],
   "source": [
    "print(\"Pipeline: Airflow -> PostgreSQL -> Spark -> Flask -> Streamlit.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

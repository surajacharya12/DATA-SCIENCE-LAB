{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Merging DataFrames\n",
        "\n",
        "This cell demonstrates how to combine two different data sources (Customers and Orders) using the `pd.merge()` function. We use an **Outer Join** to ensure no data is lost and an **Indicator** to see which table provided the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 3: Data Manipulation & Preprocessing\n",
        "\n",
        "This notebook focuses on advanced data cleaning, reshaping, and preparing data for machine learning using Pandas and Scikit-learn.\n",
        "\n",
        "| Cell # | Purpose | Key Details |\n",
        "| :--- | :--- | :--- |\n",
        "| **1** | **Merging DataFrames** | Uses `pd.merge()` to perform an Outer Join and track sources with an indicator. |\n",
        "| **2** | **Joining Data** | Demonstrates index-based joining using `.join()`. |\n",
        "| **3** | **Pivoting Data** | Transforms 'long' format to 'wide' format using `pivot_table()`. |\n",
        "| **4** | **Melting Data** | Reverses pivoting using `pd.melt()` for data normalization. |\n",
        "| **5** | **Missing Data Analysis** | Calculates counts and percentages of null values. |\n",
        "| **6** | **Basic Imputation** | Demonstrates Median and Forward Fill strategies. |\n",
        "| **7** | **Advanced Imputation** | Uses Scikit-learn's `SimpleImputer` for mean-based filling. |\n",
        "| **8** | **One-Hot Encoding** | Converts categorical names into binary columns. |\n",
        "| **9** | **Ordinal Encoding** | Maps hierarchical categories to specific integers. |\n",
        "| **10** | **Feature Scaling** | Demonstrates Normalization and Standardization. |\n",
        "| **11** | **Memory Optimization** | Downcasts numeric types to save RAM. |\n",
        "| **12** | **ML Pipelines** | Automates the entire preprocessing workflow. |\n",
        "| **13** | **Sales Data Loading** | Imports CSV data and calculates total Revenue per order. |\n",
        "| **14** | **Pivot Table Reporting** | Summarizes sales by category and city for better insight. |\n",
        "| **15** | **Data Reshaping (Melting)** | Converts wide summarized data back to long format. |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8XgqYfiVNGW",
        "outputId": "53d29bbd-f515-4418-c70e-1c9e036de3f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   customer_id     name region  order_id  amount      _merge\n",
            "0            1    Alice   East     101.0    50.0        both\n",
            "1            2      Bob   West     102.0   120.0        both\n",
            "2            2      Bob   West     103.0    80.0        both\n",
            "3            3  Charlie   East     105.0    90.0        both\n",
            "4            4    David  South       NaN     NaN   left_only\n",
            "5            5      NaN    NaN     104.0    30.0  right_only\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Example DataFrames\n",
        "customers = pd.DataFrame({\n",
        "    'customer_id': [1, 2, 3, 4],\n",
        "    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
        "    'region': ['East', 'West', 'East', 'South']\n",
        "})\n",
        "\n",
        "orders = pd.DataFrame({\n",
        "    'order_id': [101, 102, 103, 104, 105],\n",
        "    'customer_id': [1, 2, 2, 5, 3],\n",
        "    'amount': [50.0, 120.0, 80.0, 30.0, 90.0]\n",
        "})\n",
        "\n",
        "\n",
        "merged_df = pd.merge(customers, orders,\n",
        "                     on='customer_id',\n",
        "                     how='outer',\n",
        "                     indicator=True) \n",
        "\n",
        "print(merged_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Joining Data (Index-based)\n",
        "\n",
        "Unlike merging on a column, `.join()` is used to combine DataFrames based on their indexes. This is often faster and cleaner for primary-key associations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7zVHK9VVO4F",
        "outputId": "ef3c0024-6f78-49cf-9ff0-f0c4038c8eeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   customer_id     name region  order_id  amount\n",
            "0            1    Alice   East       101    50.0\n",
            "1            2      Bob   West       102   120.0\n",
            "2            2      Bob   West       103    80.0\n",
            "3            5      NaN    NaN       104    30.0\n",
            "4            3  Charlie   East       105    90.0\n"
          ]
        }
      ],
      "source": [
        "from os.path import join\n",
        "import pandas as pd\n",
        "\n",
        "customers = pd.DataFrame({\n",
        "    \"customer_id\": [1, 2, 3, 4],\n",
        "    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\n",
        "    \"region\": [\"East\", \"West\", \"East\", \"South\"]\n",
        "})\n",
        "\n",
        "orders = pd.DataFrame({\n",
        "    \"order_id\": [101, 102, 103, 104, 105],\n",
        "    \"customer_id\": [1, 2, 2, 5, 3],\n",
        "    \"amount\": [50.0, 120.0, 80.0, 30.0, 90.0]\n",
        "})\n",
        "\n",
        "join_df = customers.set_index(\"customer_id\").join(\n",
        "    orders.set_index(\"customer_id\"),\n",
        "    how=\"right\"\n",
        ").reset_index()\n",
        "\n",
        "print(join_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Pivoting Data\n",
        "\n",
        "Pivoting allows us to reshape data by turning unique values from one column into multiple new columns. Here, we turn 'Product' rows into columns to see monthly sales at a glance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Product    A   B\n",
            "Month           \n",
            "1        100  50\n",
            "2        110  60\n",
            "3        150  70\n"
          ]
        }
      ],
      "source": [
        "# Data prepared for pivoting: Sales by Product and Month\n",
        "data = {\n",
        "    'Month': [1, 1, 2, 2, 3, 3],\n",
        "    'Product': ['A', 'B', 'A', 'B', 'A', 'B'],\n",
        "    'Sales': [100, 50, 110, 60, 150, 70]\n",
        "}\n",
        "df_sales = pd.DataFrame(data)\n",
        "\n",
        "# Pivot: Turn products into columns, aggregated by month\n",
        "pivoted_df = df_sales.pivot_table(\n",
        "    index='Month',\n",
        "    columns='Product',\n",
        "    values='Sales',\n",
        "    aggfunc='sum'\n",
        ")\n",
        "\n",
        "print(pivoted_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Melting Data\n",
        "\n",
        "Melting is the inverse of pivoting. It unpivots a DataFrame from 'wide' format back to a 'long' format, which is often required for statistical modeling or plotting with libraries like Seaborn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Month Product  Sales\n",
            "0      1       A    100\n",
            "1      2       A    110\n",
            "2      3       A    150\n",
            "3      1       B     50\n",
            "4      2       B     60\n",
            "5      3       B     70\n"
          ]
        }
      ],
      "source": [
        "# Melting the pivoted_df back into a long format\n",
        "melted_df = pd.melt(pivoted_df.reset_index(),\n",
        "                    id_vars=['Month'],\n",
        "                    value_vars=['A', 'B'],\n",
        "                    var_name='Product',\n",
        "                    value_name='Sales')\n",
        "\n",
        "print(melted_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Identifying Missing Data\n",
        "\n",
        "Before cleaning data, we must identify where information is missing. This cell calculates the sum and percentage of `NaN` values per column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing Counts:\n",
            " A    1\n",
            "B    1\n",
            "C    1\n",
            "dtype: int64\n",
            "\n",
            "Missing Percentage:\n",
            " A    25.0\n",
            "B    25.0\n",
            "C    25.0\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = {'A': [1, 2, np.nan, 4], \n",
        "        'B': [5, np.nan, 7, 8], \n",
        "        'C': [9, 10, 11, np.nan]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 1. Total count of missing values per column\n",
        "print('Missing Counts:\\n', df.isnull().sum())\n",
        "\n",
        "# 2. Percentage of missing values\n",
        "print('\\nMissing Percentage:\\n', (df.isnull().sum() / len(df)) * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Basic Imputation\n",
        "\n",
        "We handle missing data using simple Pandas methods. Here we use the **Median** to fill missing numbers in column 'A' and **Forward Fill** for column 'C'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     A    B     C  A_imputed_median  C_imputed_ffill\n",
            "0  1.0  5.0   9.0               1.0              9.0\n",
            "1  2.0  NaN  10.0               2.0             10.0\n",
            "2  NaN  7.0  11.0               2.0             11.0\n",
            "3  4.0  8.0   NaN               4.0             11.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Impute Column A (Numeric) with the Median\n",
        "median_A = df['A'].median()\n",
        "df['A_imputed_median'] = df['A'].fillna(median_A)\n",
        "\n",
        "# 2. Impute Column C (Time Series) using Forward Fill (FFill)\n",
        "df['C_imputed_ffill'] = df['C'].ffill()\n",
        "\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. SimpleImputer (Scikit-learn)\n",
        "\n",
        "Scikit-learn's `SimpleImputer` provides a standardized way to fill missing values, making it easier to integrate into automated machine learning pipelines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "DataFrame after sklearn mean imputation:\n",
            "          A         B     C  A_imputed_median  C_imputed_ffill\n",
            "0  1.000000  5.000000   9.0               1.0              9.0\n",
            "1  2.000000  6.666667  10.0               2.0             10.0\n",
            "2  2.333333  7.000000  11.0               2.0             11.0\n",
            "3  4.000000  8.000000   NaN               4.0             11.0\n"
          ]
        }
      ],
      "source": [
        "# Using Scikit-learn to handle multiple columns with different strategies\n",
        "imputer_mean = SimpleImputer(strategy='mean')\n",
        "\n",
        "# Fit and transform (A and B are imputed with their respective means)\n",
        "df[['A', 'B']] = imputer_mean.fit_transform(df[['A', 'B']])\n",
        "\n",
        "print('\\nDataFrame after sklearn mean imputation:')\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. One-Hot Encoding\n",
        "\n",
        "Machine learning models cannot work with text directly. One-Hot Encoding converts categorical names (like Cities) into new columns with 0s and 1s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "One-Hot Encoded Data:\n",
            "   Price  is_London  is_NYC  is_Paris\n",
            "0    100      False    True     False\n",
            "1    200       True   False     False\n",
            "2    300      False   False      True\n",
            "3    150      False    True     False\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    'City': ['NYC', 'London', 'Paris', 'NYC'],\n",
        "    'Price': [100, 200, 300, 150]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Perform OHE\n",
        "df_ohe = pd.get_dummies(df, columns=['City'], prefix='is')\n",
        "\n",
        "print('One-Hot Encoded Data:')\n",
        "print(df_ohe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Ordinal Encoding\n",
        "\n",
        "When data has a clear order (e.g., Small < Medium < Large), we use Ordinal Encoding to convert categories into a meaningful sequence of numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ordinal Encoded Data:\n",
            "     Size  Value  Size_Encoded\n",
            "0   Small     10           0.0\n",
            "1  Medium     20           1.0\n",
            "2   Large     30           2.0\n",
            "3   Small     15           0.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import numpy as np\n",
        "\n",
        "data = {\n",
        "    'Size': ['Small', 'Medium', 'Large', 'Small'],\n",
        "    'Value': [10, 20, 30, 15]\n",
        "}\n",
        "df_ord = pd.DataFrame(data)\n",
        "\n",
        "# Define the explicit order\n",
        "size_order = ['Small', 'Medium', 'Large']\n",
        "\n",
        "encoder = OrdinalEncoder(categories=[size_order])\n",
        "\n",
        "# Fit and transform the 'Size' column\n",
        "df_ord['Size_Encoded'] = encoder.fit_transform(df_ord[['Size']])\n",
        "\n",
        "print('\\nOrdinal Encoded Data:')\n",
        "print(df_ord)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Feature Scaling\n",
        "\n",
        "This cell compares **Normalization** (scaling values to a 0-1 range) and **Standardization** (scaling to a mean of 0 and standard deviation of 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3gD2xOaYpOa",
        "outputId": "5f22abc3-58d0-4147-8534-115d8b67e543"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normalized (Min-Max):\n",
            "[[0.   0.  ]\n",
            " [0.5  0.44]\n",
            " [1.   1.  ]]\n",
            "\n",
            "Standardized (Z-Score):\n",
            "[[-1.22 -1.18]\n",
            " [ 0.   -0.09]\n",
            " [ 1.22  1.27]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([[10, 1], [20, 5], [30, 10]])\n",
        "\n",
        "# 1. Normalization (MinMaxScaler)\n",
        "scaler_norm = MinMaxScaler()\n",
        "data_normalized = scaler_norm.fit_transform(data)\n",
        "print('Normalized (Min-Max):')\n",
        "print(data_normalized.round(2))\n",
        "\n",
        "# 2. Standardization (StandardScaler)\n",
        "scaler_std = StandardScaler()\n",
        "data_standardized = scaler_std.fit_transform(data)\n",
        "print('\\nStandardized (Z-Score):')\n",
        "print(data_standardized.round(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Memory Optimization\n",
        "\n",
        "Handling big data requires efficiency. This custom function reduces memory usage by converting large numeric types to smaller byte sizes and using the `category` type for repetitive strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irqWFN22mZRh",
        "outputId": "8969c131-794f-4401-94ed-4b33f941c43f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Dtypes:\n",
            "A      int64\n",
            "B    float64\n",
            "C        str\n",
            "D        str\n",
            "dtype: object\n",
            "Original Memory Usage: 0.00 MB\n",
            "Optimized Dtypes:\n",
            "A        int8\n",
            "B     float32\n",
            "C    category\n",
            "D         str\n",
            "dtype: object\n",
            "Optimized Memory Usage: 0.00 MB\n"
          ]
        }
      ],
      "source": [
        "# 1. Optimizing Integer and Float Types\n",
        "\n",
        "def downcast_numeric(df):\n",
        "    for col in df.select_dtypes(include=['int64', 'float64']).columns:\n",
        "        # Check limits and downcast to the smallest fit\n",
        "        if 'int' in str(df[col].dtype):\n",
        "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
        "        elif 'float' in str(df[col].dtype):\n",
        "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
        "    return df\n",
        "\n",
        "# Create a sample DataFrame for demonstration\n",
        "import pandas as pd\n",
        "data = {\n",
        "    'A': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    'B': [1.1, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7, 8.8, 9.9, 10.1],\n",
        "    'C': ['apple', 'banana', 'apple', 'orange', 'banana', 'apple', 'grape', 'orange', 'banana', 'apple'],\n",
        "    'D': ['long string data', 'more long string data', 'long string data', 'unique string', 'more long string data', 'long string data', 'some other text', 'unique string', 'more long string data', 'long string data']\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(\"Original Dtypes:\")\n",
        "print(df.dtypes)\n",
        "print(f\"Original Memory Usage: {df.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
        "\n",
        "# 2. Optimizing String Types (Critical for low-cardinality data)\n",
        "\n",
        "# Column 'C' has only 5 unique values (low cardinality)\n",
        "df['C'] = df['C'].astype('category')\n",
        "\n",
        "df_optimized = downcast_numeric(df.copy())\n",
        "\n",
        "# Re-check memory usage\n",
        "optimized_mem = df_optimized.memory_usage(deep=True).sum() / (1024**2)\n",
        "\n",
        "print('Optimized Dtypes:')\n",
        "print(df_optimized.dtypes)\n",
        "print(f\"Optimized Memory Usage: {optimized_mem:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. End-to-End ML Pipelines\n",
        "\n",
        "This complex pipeline automatically handles missing data, scales numerical features, encodes categories, and trains a Logistic Regression modelâ€”all in one unified workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrZiN1OXmZqt",
        "outputId": "a9589f92-b70c-4230-bdce-e53c417f0bce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pipeline training successful.\n",
            "\n",
            "Predictions on new data: [0 1]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. Sample Data (Simulating raw input)\n",
        "data = {\n",
        "    'Age': [30, 45, np.nan, 22, 60],\n",
        "    'Income': [50000, 120000, 80000, 30000, 150000],\n",
        "    'City': ['NYC', 'London', 'Paris', 'NYC', 'London'],\n",
        "    'Target': [0, 1, 0, 1, 1]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "X = df.drop('Target', axis=1)\n",
        "y = df['Target']\n",
        "\n",
        "# --- Step 1: Define Column Groups ---\n",
        "\n",
        "numerical_features = ['Age', 'Income']\n",
        "categorical_features = ['City']\n",
        "\n",
        "# --- Step 2: Define Sub-Pipelines ---\n",
        "\n",
        "# Pipeline for Numerical Data (Impute missing, then scale)\n",
        "numerical_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Pipeline for Categorical Data (Handle missing, then OHE)\n",
        "categorical_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "\n",
        "])\n",
        "\n",
        "# --- Step 3: Combine Pipelines using ColumnTransformer ---\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_pipeline, numerical_features),\n",
        "        ('cat', categorical_pipeline, categorical_features)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# --- Step 4: Final Model Pipeline ---\n",
        "\n",
        "# The final pipeline integrates preprocessing and the model\n",
        "full_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(solver='liblinear'))\n",
        "])\n",
        "\n",
        "# --- Step 5: Training ---\n",
        "\n",
        "# The entire cleaning/scaling/training process is run in one line\n",
        "full_pipeline.fit(X, y)\n",
        "\n",
        "print(\"Pipeline training successful.\")\n",
        "\n",
        "# --- Step 6: Prediction on New Data ---\n",
        "\n",
        "new_data = pd.DataFrame({\n",
        "    'Age': [40, np.nan],\n",
        "    'Income': [60000, 95000],\n",
        "    'City': ['Paris', 'Berlin']\n",
        "})\n",
        "\n",
        "# The exact scaling and imputation rules learned from the training data are applied\n",
        "predictions = full_pipeline.predict(new_data)\n",
        "\n",
        "print(f\"\\nPredictions on new data: {predictions}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Sales Data Analysis: Loading & Exploration\n",
        "\n",
        "This section identifies how to load raw transactional data from a CSV file and perform initial inspection to understand the product categories and regional distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   order_id customer_id customer_name   product     category  price  quantity  \\\n",
            "0       101        C001         Alice    Laptop  Electronics    800         1   \n",
            "1       102        C002           Bob     Phone  Electronics    500         2   \n",
            "2       103        C001         Alice     Mouse  Accessories     20         3   \n",
            "3       104        C003       Charlie  Keyboard  Accessories     50         1   \n",
            "4       105        C002           Bob    Laptop  Electronics    800         1   \n",
            "\n",
            "        city  order_date  \n",
            "0  Kathmandu  2024-01-05  \n",
            "1   Lalitpur  2024-01-06  \n",
            "2  Kathmandu  2024-01-10  \n",
            "3  Bhaktapur  2024-02-01  \n",
            "4   Lalitpur  2024-02-05  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"sales_data.csv\")\n",
        "\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Feature Engineering: Calculating Revenue\n",
        "\n",
        "Derived features are often necessary for analysis. Here, we calculate the total Revenue for each order by multiplying the unit price with the quantity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   order_id customer_id customer_name   product     category  price  quantity  \\\n",
            "0       101        C001         Alice    Laptop  Electronics    800         1   \n",
            "1       102        C002           Bob     Phone  Electronics    500         2   \n",
            "2       103        C001         Alice     Mouse  Accessories     20         3   \n",
            "3       104        C003       Charlie  Keyboard  Accessories     50         1   \n",
            "4       105        C002           Bob    Laptop  Electronics    800         1   \n",
            "\n",
            "        city  order_date  Revenue  \n",
            "0  Kathmandu  2024-01-05      800  \n",
            "1   Lalitpur  2024-01-06     1000  \n",
            "2  Kathmandu  2024-01-10       60  \n",
            "3  Bhaktapur  2024-02-01       50  \n",
            "4   Lalitpur  2024-02-05      800  \n"
          ]
        }
      ],
      "source": [
        "df[\"Revenue\"] = df[\"price\"] * df[\"quantity\"]\n",
        "\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "category\n",
            "Accessories     200\n",
            "Electronics    3100\n",
            "Name: Revenue, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "cateegory_summary = df.groupby('category')[\"Revenue\"].sum()\n",
        "\n",
        "print(cateegory_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Pivot Table Reporting\n",
        "\n",
        "This cell creates a cross-tabulation of total Revenue indexed by Product Category and columned by City. This multi-dimensional view quickly highlights which regions dominate specific markets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "city         Bhaktapur  Kathmandu  Lalitpur\n",
            "category                                   \n",
            "Accessories         90        110         0\n",
            "Electronics          0       1300      1800\n"
          ]
        }
      ],
      "source": [
        "pivoted_df = df.pivot_table(\n",
        "    values='Revenue',\n",
        "    index='category',\n",
        "    columns='city',    \n",
        "    aggfunc='sum',\n",
        "    fill_value=0\n",
        ")\n",
        "print(pivoted_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16. Data Reshaping: Melting the Pivot Table\n",
        "\n",
        "The `.melt()` method is the inverse of pivoting. It converts the wide-format summarized table back into a long-format DataFrame, which is essential for certain visualization and database operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      category       city  Revenue\n",
            "0  Accessories  Bhaktapur       90\n",
            "1  Electronics  Bhaktapur        0\n",
            "2  Accessories  Kathmandu      110\n",
            "3  Electronics  Kathmandu     1300\n",
            "4  Accessories   Lalitpur        0\n"
          ]
        }
      ],
      "source": [
        "melted_df = pivoted_df.reset_index().melt(\n",
        "    id_vars='category',\n",
        "    var_name='city',\n",
        "    value_name='Revenue'\n",
        ")\n",
        "print(melted_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   order_id   product  quantity     category  price customer_id  order_date  \\\n",
            "0       101    Laptop         1  Electronics    800        C001  2024-01-05   \n",
            "1       102     Phone         2  Electronics    500        C002  2024-01-06   \n",
            "2       103     Mouse         3  Accessories     20        C001  2024-01-10   \n",
            "3       104  Keyboard         1  Accessories     50        C003  2024-02-01   \n",
            "4       105    Laptop         1  Electronics    800        C002  2024-02-05   \n",
            "\n",
            "  customer_name       city  \n",
            "0         Alice  Kathmandu  \n",
            "1           Bob   Lalitpur  \n",
            "2         Alice  Kathmandu  \n",
            "3       Charlie  Bhaktapur  \n",
            "4           Bob   Lalitpur  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "customers = df[[\"customer_id\", \"customer_name\", \"city\"]].drop_duplicates()\n",
        "products = df[[\"product\", \"category\", \"price\"]].drop_duplicates()\n",
        "orders = df[[\"order_id\", \"customer_id\", \"order_date\"]].drop_duplicates()\n",
        "order_details = df[[\"order_id\", \"product\", \"quantity\"]].drop_duplicates()\n",
        "\n",
        "merged_df = order_details.merge(products, on=\"product\", how=\"left\") \\\n",
        "                         .merge(orders, on=\"order_id\", how=\"left\") \\\n",
        "                         .merge(customers, on=\"customer_id\", how=\"left\")\n",
        "\n",
        "print(merged_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Log Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Values  Log_Values\n",
            "0       1    0.000000\n",
            "1       2    0.301030\n",
            "2       3    0.477121\n",
            "3       4    0.602060\n",
            "4       5    0.698970\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = {\n",
        "    'Values': [1, 2, 3, 4, 5]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df['Log_Values'] = np.log10(df['Values'])\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Means (Col A, Col B): [200.   2.]\n",
            "Scaled Means (should be close to 0): [0. 0.]\n",
            "Scaled Data:\n",
            " [[-1.22474487 -1.22474487]\n",
            " [ 0.          0.        ]\n",
            " [ 1.22474487  1.22474487]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([\n",
        "    [100, 1],\n",
        "    [200, 2],\n",
        "    [300, 3]\n",
        "])\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\"Original Means (Col A, Col B):\", np.mean(X, axis=0))\n",
        "print(\"Scaled Means (should be close to 0):\", np.mean(X_scaled, axis=0).round(2))\n",
        "print(\"Scaled Data:\\n\", X_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## One-Hot Encoding (OHE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Fruit_Banana  Fruit_Orange\n",
            "0         False         False\n",
            "1          True         False\n",
            "2         False         False\n",
            "3         False          True\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'Fruit': ['Apple', 'Banana', 'Apple', 'Orange']})\n",
        "\n",
        "df_encoded = pd.get_dummies(df, columns=['Fruit'], drop_first=True)\n",
        "print(df_encoded)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    employee_id         department     region        education gender  \\\n",
            "0          8724         Technology  region_26        Bachelors      m   \n",
            "1         74430                 HR   region_4        Bachelors      f   \n",
            "2         72255  Sales & Marketing  region_13        Bachelors      m   \n",
            "3         38562        Procurement   region_2        Bachelors      f   \n",
            "4         64486            Finance  region_29        Bachelors      m   \n",
            "5         46232        Procurement   region_7        Bachelors      m   \n",
            "6         54542            Finance   region_2        Bachelors      m   \n",
            "7         67269          Analytics  region_22        Bachelors      m   \n",
            "8         66174         Technology   region_7  Masters & above      m   \n",
            "9         76303         Technology  region_22        Bachelors      m   \n",
            "10        60245  Sales & Marketing  region_16        Bachelors      m   \n",
            "11        42639  Sales & Marketing  region_17  Masters & above      m   \n",
            "12        30963  Sales & Marketing   region_4  Masters & above      f   \n",
            "13        54055          Analytics  region_24        Bachelors      m   \n",
            "14        42996         Operations  region_11        Bachelors      m   \n",
            "15        12737  Sales & Marketing   region_7        Bachelors      m   \n",
            "16        27561         Operations  region_27        Bachelors      f   \n",
            "17        26622  Sales & Marketing  region_17        Bachelors      m   \n",
            "18        31582        Procurement   region_7        Bachelors      f   \n",
            "19        29793        Procurement  region_27        Bachelors      m   \n",
            "\n",
            "   recruitment_channel  no_of_trainings  age  previous_year_rating  \\\n",
            "0             sourcing                1   24                   NaN   \n",
            "1                other                1   31                   3.0   \n",
            "2                other                1   31                   1.0   \n",
            "3                other                3   31                   2.0   \n",
            "4             sourcing                1   30                   4.0   \n",
            "5             sourcing                1   36                   3.0   \n",
            "6                other                1   33                   5.0   \n",
            "7             sourcing                2   36                   3.0   \n",
            "8                other                1   51                   4.0   \n",
            "9             sourcing                1   29                   5.0   \n",
            "10            sourcing                2   40                   5.0   \n",
            "11            sourcing                1   40                   3.0   \n",
            "12               other                1   34                   3.0   \n",
            "13               other                1   37                   3.0   \n",
            "14            sourcing                1   30                   5.0   \n",
            "15            sourcing                1   31                   4.0   \n",
            "16            sourcing                1   26                   5.0   \n",
            "17            sourcing                1   40                   5.0   \n",
            "18               other                1   49                   3.0   \n",
            "19               other                1   27                   2.0   \n",
            "\n",
            "    length_of_service  KPIs_met_more_than_80  awards_won  avg_training_score  \n",
            "0                   1                      1           0                  77  \n",
            "1                   5                      0           0                  51  \n",
            "2                   4                      0           0                  47  \n",
            "3                   9                      0           0                  65  \n",
            "4                   7                      0           0                  61  \n",
            "5                   2                      0           0                  68  \n",
            "6                   3                      1           0                  57  \n",
            "7                   3                      0           0                  85  \n",
            "8                  11                      0           0                  75  \n",
            "9                   2                      1           0                  76  \n",
            "10                 12                      1           0                  50  \n",
            "11                 10                      0           0                  46  \n",
            "12                  4                      0           0                  52  \n",
            "13                 10                      0           0                  82  \n",
            "14                  6                      1           0                  58  \n",
            "15                  4                      1           0                  47  \n",
            "16                  3                      0           0                  56  \n",
            "17                  6                      1           0                  50  \n",
            "18                  7                      1           0                  64  \n",
            "19                  5                      0           0                  65  \n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"employee.csv\")\n",
        "\n",
        "print(df.head(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    employee_id         department     region        education gender  \\\n",
            "0          8724         Technology  region_26        Bachelors      m   \n",
            "1         74430                 HR   region_4        Bachelors      f   \n",
            "2         72255  Sales & Marketing  region_13        Bachelors      m   \n",
            "3         38562        Procurement   region_2        Bachelors      f   \n",
            "4         64486            Finance  region_29        Bachelors      m   \n",
            "5         46232        Procurement   region_7        Bachelors      m   \n",
            "6         54542            Finance   region_2        Bachelors      m   \n",
            "7         67269          Analytics  region_22        Bachelors      m   \n",
            "8         66174         Technology   region_7  Masters & above      m   \n",
            "9         76303         Technology  region_22        Bachelors      m   \n",
            "10        60245  Sales & Marketing  region_16        Bachelors      m   \n",
            "11        42639  Sales & Marketing  region_17  Masters & above      m   \n",
            "12        30963  Sales & Marketing   region_4  Masters & above      f   \n",
            "13        54055          Analytics  region_24        Bachelors      m   \n",
            "14        42996         Operations  region_11        Bachelors      m   \n",
            "15        12737  Sales & Marketing   region_7        Bachelors      m   \n",
            "16        27561         Operations  region_27        Bachelors      f   \n",
            "17        26622  Sales & Marketing  region_17        Bachelors      m   \n",
            "18        31582        Procurement   region_7        Bachelors      f   \n",
            "19        29793        Procurement  region_27        Bachelors      m   \n",
            "\n",
            "   recruitment_channel  no_of_trainings  age  previous_year_rating  \\\n",
            "0             sourcing                1   24                   NaN   \n",
            "1                other                1   31                   3.0   \n",
            "2                other                1   31                   1.0   \n",
            "3                other                3   31                   2.0   \n",
            "4             sourcing                1   30                   4.0   \n",
            "5             sourcing                1   36                   3.0   \n",
            "6                other                1   33                   5.0   \n",
            "7             sourcing                2   36                   3.0   \n",
            "8                other                1   51                   4.0   \n",
            "9             sourcing                1   29                   5.0   \n",
            "10            sourcing                2   40                   5.0   \n",
            "11            sourcing                1   40                   3.0   \n",
            "12               other                1   34                   3.0   \n",
            "13               other                1   37                   3.0   \n",
            "14            sourcing                1   30                   5.0   \n",
            "15            sourcing                1   31                   4.0   \n",
            "16            sourcing                1   26                   5.0   \n",
            "17            sourcing                1   40                   5.0   \n",
            "18               other                1   49                   3.0   \n",
            "19               other                1   27                   2.0   \n",
            "\n",
            "    length_of_service  KPIs_met_more_than_80  awards_won  avg_training_score  \n",
            "0                   1                      1           0                  77  \n",
            "1                   5                      0           0                  51  \n",
            "2                   4                      0           0                  47  \n",
            "3                   9                      0           0                  65  \n",
            "4                   7                      0           0                  61  \n",
            "5                   2                      0           0                  68  \n",
            "6                   3                      1           0                  57  \n",
            "7                   3                      0           0                  85  \n",
            "8                  11                      0           0                  75  \n",
            "9                   2                      1           0                  76  \n",
            "10                 12                      1           0                  50  \n",
            "11                 10                      0           0                  46  \n",
            "12                  4                      0           0                  52  \n",
            "13                 10                      0           0                  82  \n",
            "14                  6                      1           0                  58  \n",
            "15                  4                      1           0                  47  \n",
            "16                  3                      0           0                  56  \n",
            "17                  6                      1           0                  50  \n",
            "18                  7                      1           0                  64  \n",
            "19                  5                      0           0                  65  \n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"employee.csv\")\n",
        "\n",
        "print(df.head(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>employee_id</th>\n",
              "      <th>no_of_trainings</th>\n",
              "      <th>age</th>\n",
              "      <th>previous_year_rating</th>\n",
              "      <th>length_of_service</th>\n",
              "      <th>KPIs_met_more_than_80</th>\n",
              "      <th>awards_won</th>\n",
              "      <th>avg_training_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>17417.000000</td>\n",
              "      <td>17417.000000</td>\n",
              "      <td>17417.000000</td>\n",
              "      <td>16054.000000</td>\n",
              "      <td>17417.000000</td>\n",
              "      <td>17417.000000</td>\n",
              "      <td>17417.000000</td>\n",
              "      <td>17417.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>39083.491129</td>\n",
              "      <td>1.250732</td>\n",
              "      <td>34.807774</td>\n",
              "      <td>3.345459</td>\n",
              "      <td>5.801860</td>\n",
              "      <td>0.358845</td>\n",
              "      <td>0.023368</td>\n",
              "      <td>63.176322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>22707.024087</td>\n",
              "      <td>0.595692</td>\n",
              "      <td>7.694046</td>\n",
              "      <td>1.265386</td>\n",
              "      <td>4.175533</td>\n",
              "      <td>0.479675</td>\n",
              "      <td>0.151074</td>\n",
              "      <td>13.418179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>39.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>19281.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>51.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>39122.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>33.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>60.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>58838.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>75.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>78295.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>34.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>99.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        employee_id  no_of_trainings           age  previous_year_rating  \\\n",
              "count  17417.000000     17417.000000  17417.000000          16054.000000   \n",
              "mean   39083.491129         1.250732     34.807774              3.345459   \n",
              "std    22707.024087         0.595692      7.694046              1.265386   \n",
              "min        3.000000         1.000000     20.000000              1.000000   \n",
              "25%    19281.000000         1.000000     29.000000              3.000000   \n",
              "50%    39122.000000         1.000000     33.000000              3.000000   \n",
              "75%    58838.000000         1.000000     39.000000              4.000000   \n",
              "max    78295.000000         9.000000     60.000000              5.000000   \n",
              "\n",
              "       length_of_service  KPIs_met_more_than_80    awards_won  \\\n",
              "count       17417.000000           17417.000000  17417.000000   \n",
              "mean            5.801860               0.358845      0.023368   \n",
              "std             4.175533               0.479675      0.151074   \n",
              "min             1.000000               0.000000      0.000000   \n",
              "25%             3.000000               0.000000      0.000000   \n",
              "50%             5.000000               0.000000      0.000000   \n",
              "75%             7.000000               1.000000      0.000000   \n",
              "max            34.000000               1.000000      1.000000   \n",
              "\n",
              "       avg_training_score  \n",
              "count        17417.000000  \n",
              "mean            63.176322  \n",
              "std             13.418179  \n",
              "min             39.000000  \n",
              "25%             51.000000  \n",
              "50%             60.000000  \n",
              "75%             75.000000  \n",
              "max             99.000000  "
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(17417, 13)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "employee_id                 0\n",
            "department                  0\n",
            "region                      0\n",
            "education                 771\n",
            "gender                      0\n",
            "recruitment_channel         0\n",
            "no_of_trainings             0\n",
            "age                         0\n",
            "previous_year_rating     1363\n",
            "length_of_service           0\n",
            "KPIs_met_more_than_80       0\n",
            "awards_won                  0\n",
            "avg_training_score          0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "employee_id              0.000000\n",
            "department               0.000000\n",
            "region                   0.000000\n",
            "education                4.426710\n",
            "gender                   0.000000\n",
            "recruitment_channel      0.000000\n",
            "no_of_trainings          0.000000\n",
            "age                      0.000000\n",
            "previous_year_rating     7.825688\n",
            "length_of_service        0.000000\n",
            "KPIs_met_more_than_80    0.000000\n",
            "awards_won               0.000000\n",
            "avg_training_score       0.000000\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "print((df.isnull().sum()/len(df))*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/nb/_rv6f3v52rq4wx3ktv25ltvr0000gn/T/ipykernel_77547/3909758819.py:1: ChainedAssignmentError: A value is being set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "Such inplace method never works to update the original DataFrame or Series, because the intermediate object on which we are setting values always behaves as a copy (due to Copy-on-Write).\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' instead, to perform the operation inplace on the original object, or try to avoid an inplace operation using 'df[col] = df[col].method(value)'.\n",
            "\n",
            "See the documentation for a more detailed explanation: https://pandas.pydata.org/pandas-docs/stable/user_guide/copy_on_write.html\n",
            "  df['previous_year_rating'].fillna(df['previous_year_rating'].median(), inplace=True)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0        3.0\n",
              "1        3.0\n",
              "2        1.0\n",
              "3        2.0\n",
              "4        4.0\n",
              "        ... \n",
              "17412    5.0\n",
              "17413    1.0\n",
              "17414    1.0\n",
              "17415    1.0\n",
              "17416    5.0\n",
              "Name: previous_year_rating, Length: 17417, dtype: float64"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['previous_year_rating'].fillna(df['previous_year_rating'].median(), inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/nb/_rv6f3v52rq4wx3ktv25ltvr0000gn/T/ipykernel_77547/2668662719.py:1: ChainedAssignmentError: A value is being set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "Such inplace method never works to update the original DataFrame or Series, because the intermediate object on which we are setting values always behaves as a copy (due to Copy-on-Write).\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' instead, to perform the operation inplace on the original object, or try to avoid an inplace operation using 'df[col] = df[col].method(value)'.\n",
            "\n",
            "See the documentation for a more detailed explanation: https://pandas.pydata.org/pandas-docs/stable/user_guide/copy_on_write.html\n",
            "  df['education'].fillna(df['education'].mode()[0], inplace=True)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0        Bachelors\n",
              "1        Bachelors\n",
              "2        Bachelors\n",
              "3        Bachelors\n",
              "4        Bachelors\n",
              "           ...    \n",
              "17412    Bachelors\n",
              "17413    Bachelors\n",
              "17414    Bachelors\n",
              "17415    Bachelors\n",
              "17416    Bachelors\n",
              "Name: education, Length: 17417, dtype: str"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['education'].fillna(df['education'].mode()[0], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "categorical_columns = ['department', 'region', 'education', 'gender', 'recruitment_channel']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "numerical_columns = ['employee_id', 'no_of_trainings', 'age', 'previous_year_rating', \n",
        "                     'length_of_service', 'KPIs_met_more_than_80', 'awards_won', 'avg_training_score']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Numerical columns scaled with StandardScaler:\n",
            "   employee_id  no_of_trainings       age  previous_year_rating  \\\n",
            "0    -1.337047        -0.420921 -1.404733                   NaN   \n",
            "1     1.556678        -0.420921 -0.494913             -0.273015   \n",
            "2     1.460890        -0.420921 -0.494913             -1.853610   \n",
            "3    -0.022967         2.936614 -0.494913             -1.063313   \n",
            "4     1.118739        -0.420921 -0.624887              0.517282   \n",
            "\n",
            "   length_of_service  KPIs_met_more_than_80  awards_won  avg_training_score  \n",
            "0          -1.150032               1.336682   -0.154684            1.030250  \n",
            "1          -0.192043              -0.748121   -0.154684           -0.907476  \n",
            "2          -0.431541              -0.748121   -0.154684           -1.205587  \n",
            "3           0.765946              -0.748121   -0.154684            0.135915  \n",
            "4           0.286951              -0.748121   -0.154684           -0.162197  \n",
            "\n",
            "Scaled data statistics:\n",
            "        employee_id  no_of_trainings           age  previous_year_rating  \\\n",
            "count  1.741700e+04     1.741700e+04  1.741700e+04          1.605400e+04   \n",
            "mean  -9.709432e-17     1.656315e-16 -3.210640e-16         -4.381695e-17   \n",
            "std    1.000029e+00     1.000029e+00  1.000029e+00          1.000031e+00   \n",
            "min   -1.721124e+00    -4.209208e-01 -1.924631e+00         -1.853610e+00   \n",
            "25%   -8.721116e-01    -4.209208e-01 -7.548617e-01         -2.730154e-01   \n",
            "50%    1.695950e-03    -4.209208e-01 -2.349643e-01         -2.730154e-01   \n",
            "75%    8.699985e-01    -4.209208e-01  5.448819e-01          5.172820e-01   \n",
            "max    1.726894e+00     1.300922e+01  3.274343e+00          1.307579e+00   \n",
            "\n",
            "       length_of_service  KPIs_met_more_than_80    awards_won  \\\n",
            "count       1.741700e+04           1.741700e+04  1.741700e+04   \n",
            "mean        5.711430e-17           1.197361e-16 -3.508450e-17   \n",
            "std         1.000029e+00           1.000029e+00  1.000029e+00   \n",
            "min        -1.150032e+00          -7.481208e-01 -1.546839e-01   \n",
            "25%        -6.710379e-01          -7.481208e-01 -1.546839e-01   \n",
            "50%        -1.920434e-01          -7.481208e-01 -1.546839e-01   \n",
            "75%         2.869512e-01           1.336682e+00 -1.546839e-01   \n",
            "max         6.753378e+00           1.336682e+00  6.464798e+00   \n",
            "\n",
            "       avg_training_score  \n",
            "count        1.741700e+04  \n",
            "mean         1.403380e-16  \n",
            "std          1.000029e+00  \n",
            "min         -1.801811e+00  \n",
            "25%         -9.074757e-01  \n",
            "50%         -2.367246e-01  \n",
            "75%          8.811939e-01  \n",
            "max          2.669864e+00  \n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import pandas as pd\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "df_processed = df.copy()\n",
        "\n",
        "\n",
        "df_processed[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
        "\n",
        "print(\"Numerical columns scaled with StandardScaler:\")\n",
        "print(df_processed[numerical_columns].head())\n",
        "print(\"\\nScaled data statistics:\")\n",
        "print(df_processed[numerical_columns].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Categorical columns encoded with LabelEncoder:\n",
            "   department  region  education  gender  recruitment_channel\n",
            "0           8      18          0       1                    2\n",
            "1           2      28          0       0                    0\n",
            "2           7       4          0       1                    0\n",
            "3           5      11          0       0                    0\n",
            "4           1      21          0       1                    2\n",
            "\n",
            "Encoding mappings:\n",
            "\n",
            "department: {'Analytics': np.int64(0), 'Finance': np.int64(1), 'HR': np.int64(2), 'Legal': np.int64(3), 'Operations': np.int64(4), 'Procurement': np.int64(5), 'R&D': np.int64(6), 'Sales & Marketing': np.int64(7), 'Technology': np.int64(8)}\n",
            "\n",
            "region: {'region_1': np.int64(0), 'region_10': np.int64(1), 'region_11': np.int64(2), 'region_12': np.int64(3), 'region_13': np.int64(4), 'region_14': np.int64(5), 'region_15': np.int64(6), 'region_16': np.int64(7), 'region_17': np.int64(8), 'region_18': np.int64(9), 'region_19': np.int64(10), 'region_2': np.int64(11), 'region_20': np.int64(12), 'region_21': np.int64(13), 'region_22': np.int64(14), 'region_23': np.int64(15), 'region_24': np.int64(16), 'region_25': np.int64(17), 'region_26': np.int64(18), 'region_27': np.int64(19), 'region_28': np.int64(20), 'region_29': np.int64(21), 'region_3': np.int64(22), 'region_30': np.int64(23), 'region_31': np.int64(24), 'region_32': np.int64(25), 'region_33': np.int64(26), 'region_34': np.int64(27), 'region_4': np.int64(28), 'region_5': np.int64(29), 'region_6': np.int64(30), 'region_7': np.int64(31), 'region_8': np.int64(32), 'region_9': np.int64(33)}\n",
            "\n",
            "education: {'Bachelors': np.int64(0), 'Below Secondary': np.int64(1), 'Masters & above': np.int64(2), nan: np.int64(3)}\n",
            "\n",
            "gender: {'f': np.int64(0), 'm': np.int64(1)}\n",
            "\n",
            "recruitment_channel: {'other': np.int64(0), 'referred': np.int64(1), 'sourcing': np.int64(2)}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "label_encoders = {}\n",
        "\n",
        "for col in categorical_columns:\n",
        "    le = LabelEncoder()\n",
        "    df_processed[col] = le.fit_transform(df[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "\n",
        "print(\"Categorical columns encoded with LabelEncoder:\")\n",
        "print(df_processed[categorical_columns].head())\n",
        "print(\"\\nEncoding mappings:\")\n",
        "for col, encoder in label_encoders.items():\n",
        "    print(f\"\\n{col}: {dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Preprocessed DataFrame:\n",
            "   employee_id  department  region  education  gender  recruitment_channel  \\\n",
            "0    -1.337047           8      18          0       1                    2   \n",
            "1     1.556678           2      28          0       0                    0   \n",
            "2     1.460890           7       4          0       1                    0   \n",
            "3    -0.022967           5      11          0       0                    0   \n",
            "4     1.118739           1      21          0       1                    2   \n",
            "\n",
            "   no_of_trainings       age  previous_year_rating  length_of_service  \\\n",
            "0        -0.420921 -1.404733                   NaN          -1.150032   \n",
            "1        -0.420921 -0.494913             -0.273015          -0.192043   \n",
            "2        -0.420921 -0.494913             -1.853610          -0.431541   \n",
            "3         2.936614 -0.494913             -1.063313           0.765946   \n",
            "4        -0.420921 -0.624887              0.517282           0.286951   \n",
            "\n",
            "   KPIs_met_more_than_80  awards_won  avg_training_score  \n",
            "0               1.336682   -0.154684            1.030250  \n",
            "1              -0.748121   -0.154684           -0.907476  \n",
            "2              -0.748121   -0.154684           -1.205587  \n",
            "3              -0.748121   -0.154684            0.135915  \n",
            "4              -0.748121   -0.154684           -0.162197  \n",
            "\n",
            "Shape: (17417, 13)\n",
            "\n",
            "Data Types:\n",
            "employee_id              float64\n",
            "department                 int64\n",
            "region                     int64\n",
            "education                  int64\n",
            "gender                     int64\n",
            "recruitment_channel        int64\n",
            "no_of_trainings          float64\n",
            "age                      float64\n",
            "previous_year_rating     float64\n",
            "length_of_service        float64\n",
            "KPIs_met_more_than_80    float64\n",
            "awards_won               float64\n",
            "avg_training_score       float64\n",
            "dtype: object\n",
            "\n",
            "Summary Statistics (All Columns):\n",
            "        employee_id    department        region     education        gender  \\\n",
            "count  1.741700e+04  17417.000000  17417.000000  17417.000000  17417.000000   \n",
            "mean  -9.709432e-17      4.969283     15.490440      0.705116      0.707010   \n",
            "std    1.000029e+00      2.507964      8.757688      1.014634      0.455147   \n",
            "min   -1.721124e+00      0.000000      0.000000      0.000000      0.000000   \n",
            "25%   -8.721116e-01      4.000000     11.000000      0.000000      0.000000   \n",
            "50%    1.695950e-03      5.000000     14.000000      0.000000      1.000000   \n",
            "75%    8.699985e-01      7.000000     22.000000      2.000000      1.000000   \n",
            "max    1.726894e+00      8.000000     33.000000      3.000000      1.000000   \n",
            "\n",
            "       recruitment_channel  no_of_trainings           age  \\\n",
            "count         17417.000000     1.741700e+04  1.741700e+04   \n",
            "mean              0.862089     1.656315e-16 -3.210640e-16   \n",
            "std               0.981242     1.000029e+00  1.000029e+00   \n",
            "min               0.000000    -4.209208e-01 -1.924631e+00   \n",
            "25%               0.000000    -4.209208e-01 -7.548617e-01   \n",
            "50%               0.000000    -4.209208e-01 -2.349643e-01   \n",
            "75%               2.000000    -4.209208e-01  5.448819e-01   \n",
            "max               2.000000     1.300922e+01  3.274343e+00   \n",
            "\n",
            "       previous_year_rating  length_of_service  KPIs_met_more_than_80  \\\n",
            "count          1.605400e+04       1.741700e+04           1.741700e+04   \n",
            "mean          -4.381695e-17       5.711430e-17           1.197361e-16   \n",
            "std            1.000031e+00       1.000029e+00           1.000029e+00   \n",
            "min           -1.853610e+00      -1.150032e+00          -7.481208e-01   \n",
            "25%           -2.730154e-01      -6.710379e-01          -7.481208e-01   \n",
            "50%           -2.730154e-01      -1.920434e-01          -7.481208e-01   \n",
            "75%            5.172820e-01       2.869512e-01           1.336682e+00   \n",
            "max            1.307579e+00       6.753378e+00           1.336682e+00   \n",
            "\n",
            "         awards_won  avg_training_score  \n",
            "count  1.741700e+04        1.741700e+04  \n",
            "mean  -3.508450e-17        1.403380e-16  \n",
            "std    1.000029e+00        1.000029e+00  \n",
            "min   -1.546839e-01       -1.801811e+00  \n",
            "25%   -1.546839e-01       -9.074757e-01  \n",
            "50%   -1.546839e-01       -2.367246e-01  \n",
            "75%   -1.546839e-01        8.811939e-01  \n",
            "max    6.464798e+00        2.669864e+00  \n"
          ]
        }
      ],
      "source": [
        "print(\"Final Preprocessed DataFrame:\")\n",
        "print(df_processed.head())\n",
        "print(f\"\\nShape: {df_processed.shape}\")\n",
        "print(f\"\\nData Types:\\n{df_processed.dtypes}\")\n",
        "print(\"\\nSummary Statistics (All Columns):\")\n",
        "print(df_processed.describe())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8XgqYfiVNGW",
        "outputId": "53d29bbd-f515-4418-c70e-1c9e036de3f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   customer_id     name region  order_id  amount      _merge\n",
            "0            1    Alice   East     101.0    50.0        both\n",
            "1            2      Bob   West     102.0   120.0        both\n",
            "2            2      Bob   West     103.0    80.0        both\n",
            "3            3  Charlie   East     105.0    90.0        both\n",
            "4            4    David  South       NaN     NaN   left_only\n",
            "5            5      NaN    NaN     104.0    30.0  right_only\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Example DataFrames\n",
        "customers = pd.DataFrame({\n",
        "    'customer_id': [1, 2, 3, 4],\n",
        "    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
        "    'region': ['East', 'West', 'East', 'South']\n",
        "})\n",
        "\n",
        "orders = pd.DataFrame({\n",
        "    'order_id': [101, 102, 103, 104, 105],\n",
        "    'customer_id': [1, 2, 2, 5, 3],\n",
        "    'amount': [50.0, 120.0, 80.0, 30.0, 90.0]\n",
        "})\n",
        "\n",
        "\n",
        "merged_df = pd.merge(customers, orders,\n",
        "                     on='customer_id',\n",
        "                     how='outer',\n",
        "                     indicator=True) \n",
        "\n",
        "print(merged_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7zVHK9VVO4F",
        "outputId": "ef3c0024-6f78-49cf-9ff0-f0c4038c8eeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   customer_id     name region  order_id  amount\n",
            "0            1    Alice   East       101    50.0\n",
            "1            2      Bob   West       102   120.0\n",
            "2            2      Bob   West       103    80.0\n",
            "3            5      NaN    NaN       104    30.0\n",
            "4            3  Charlie   East       105    90.0\n"
          ]
        }
      ],
      "source": [
        "from os.path import join\n",
        "import pandas as pd\n",
        "\n",
        "customers = pd.DataFrame({\n",
        "    \"customer_id\": [1, 2, 3, 4],\n",
        "    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\n",
        "    \"region\": [\"East\", \"West\", \"East\", \"South\"]\n",
        "})\n",
        "\n",
        "orders = pd.DataFrame({\n",
        "    \"order_id\": [101, 102, 103, 104, 105],\n",
        "    \"customer_id\": [1, 2, 2, 5, 3],\n",
        "    \"amount\": [50.0, 120.0, 80.0, 30.0, 90.0]\n",
        "})\n",
        "\n",
        "join_df = customers.set_index(\"customer_id\").join(\n",
        "    orders.set_index(\"customer_id\"),\n",
        "    how=\"right\"\n",
        ").reset_index()\n",
        "\n",
        "print(join_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Product    A   B\n",
            "Month           \n",
            "1        100  50\n",
            "2        110  60\n",
            "3        150  70\n"
          ]
        }
      ],
      "source": [
        "# Data prepared for pivoting: Sales by Product and Month\n",
        "data = {\n",
        "    'Month': [1, 1, 2, 2, 3, 3],\n",
        "    'Product': ['A', 'B', 'A', 'B', 'A', 'B'],\n",
        "    'Sales': [100, 50, 110, 60, 150, 70]\n",
        "}\n",
        "df_sales = pd.DataFrame(data)\n",
        "\n",
        "# Pivot: Turn products into columns, aggregated by month\n",
        "pivoted_df = df_sales.pivot_table(\n",
        "    index='Month',\n",
        "    columns='Product',\n",
        "    values='Sales',\n",
        "    aggfunc='sum'\n",
        ")\n",
        "\n",
        "print(pivoted_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Month Product  Sales\n",
            "0      1       A    100\n",
            "1      2       A    110\n",
            "2      3       A    150\n",
            "3      1       B     50\n",
            "4      2       B     60\n",
            "5      3       B     70\n"
          ]
        }
      ],
      "source": [
        "# Melting the pivoted_df back into a long format\n",
        "melted_df = pd.melt(pivoted_df.reset_index(),\n",
        "                    id_vars=['Month'],\n",
        "                    value_vars=['A', 'B'],\n",
        "                    var_name='Product',\n",
        "                    value_name='Sales')\n",
        "\n",
        "print(melted_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing Counts:\n",
            " A    1\n",
            "B    1\n",
            "C    1\n",
            "dtype: int64\n",
            "\n",
            "Missing Percentage:\n",
            " A    25.0\n",
            "B    25.0\n",
            "C    25.0\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = {'A': [1, 2, np.nan, 4], \n",
        "        'B': [5, np.nan, 7, 8], \n",
        "        'C': [9, 10, 11, np.nan]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 1. Total count of missing values per column\n",
        "print('Missing Counts:\\n', df.isnull().sum())\n",
        "\n",
        "# 2. Percentage of missing values\n",
        "print('\\nMissing Percentage:\\n', (df.isnull().sum() / len(df)) * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     A    B     C  A_imputed_median  C_imputed_ffill\n",
            "0  1.0  5.0   9.0               1.0              9.0\n",
            "1  2.0  NaN  10.0               2.0             10.0\n",
            "2  NaN  7.0  11.0               2.0             11.0\n",
            "3  4.0  8.0   NaN               4.0             11.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Impute Column A (Numeric) with the Median\n",
        "median_A = df['A'].median()\n",
        "df['A_imputed_median'] = df['A'].fillna(median_A)\n",
        "\n",
        "# 2. Impute Column C (Time Series) using Forward Fill (FFill)\n",
        "df['C_imputed_ffill'] = df['C'].ffill()\n",
        "\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "DataFrame after sklearn mean imputation:\n",
            "          A         B     C  A_imputed_median  C_imputed_ffill\n",
            "0  1.000000  5.000000   9.0               1.0              9.0\n",
            "1  2.000000  6.666667  10.0               2.0             10.0\n",
            "2  2.333333  7.000000  11.0               2.0             11.0\n",
            "3  4.000000  8.000000   NaN               4.0             11.0\n"
          ]
        }
      ],
      "source": [
        "# Using Scikit-learn to handle multiple columns with different strategies\n",
        "imputer_mean = SimpleImputer(strategy='mean')\n",
        "\n",
        "# Fit and transform (A and B are imputed with their respective means)\n",
        "df[['A', 'B']] = imputer_mean.fit_transform(df[['A', 'B']])\n",
        "\n",
        "print('\\nDataFrame after sklearn mean imputation:')\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "One-Hot Encoded Data:\n",
            "   Price  is_London  is_NYC  is_Paris\n",
            "0    100      False    True     False\n",
            "1    200       True   False     False\n",
            "2    300      False   False      True\n",
            "3    150      False    True     False\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    'City': ['NYC', 'London', 'Paris', 'NYC'],\n",
        "    'Price': [100, 200, 300, 150]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Perform OHE\n",
        "df_ohe = pd.get_dummies(df, columns=['City'], prefix='is')\n",
        "\n",
        "print('One-Hot Encoded Data:')\n",
        "print(df_ohe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ordinal Encoded Data:\n",
            "     Size  Value  Size_Encoded\n",
            "0   Small     10           0.0\n",
            "1  Medium     20           1.0\n",
            "2   Large     30           2.0\n",
            "3   Small     15           0.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import numpy as np\n",
        "\n",
        "data = {\n",
        "    'Size': ['Small', 'Medium', 'Large', 'Small'],\n",
        "    'Value': [10, 20, 30, 15]\n",
        "}\n",
        "df_ord = pd.DataFrame(data)\n",
        "\n",
        "# Define the explicit order\n",
        "size_order = ['Small', 'Medium', 'Large']\n",
        "\n",
        "encoder = OrdinalEncoder(categories=[size_order])\n",
        "\n",
        "# Fit and transform the 'Size' column\n",
        "df_ord['Size_Encoded'] = encoder.fit_transform(df_ord[['Size']])\n",
        "\n",
        "print('\\nOrdinal Encoded Data:')\n",
        "print(df_ord)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3gD2xOaYpOa",
        "outputId": "5f22abc3-58d0-4147-8534-115d8b67e543"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normalized (Min-Max):\n",
            "[[0.   0.  ]\n",
            " [0.5  0.44]\n",
            " [1.   1.  ]]\n",
            "\n",
            "Standardized (Z-Score):\n",
            "[[-1.22 -1.18]\n",
            " [ 0.   -0.09]\n",
            " [ 1.22  1.27]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([[10, 1], [20, 5], [30, 10]])\n",
        "\n",
        "# 1. Normalization (MinMaxScaler)\n",
        "scaler_norm = MinMaxScaler()\n",
        "data_normalized = scaler_norm.fit_transform(data)\n",
        "print('Normalized (Min-Max):')\n",
        "print(data_normalized.round(2))\n",
        "\n",
        "# 2. Standardization (StandardScaler)\n",
        "scaler_std = StandardScaler()\n",
        "data_standardized = scaler_std.fit_transform(data)\n",
        "print('\\nStandardized (Z-Score):')\n",
        "print(data_standardized.round(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irqWFN22mZRh",
        "outputId": "8969c131-794f-4401-94ed-4b33f941c43f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Dtypes:\n",
            "A      int64\n",
            "B    float64\n",
            "C        str\n",
            "D        str\n",
            "dtype: object\n",
            "Original Memory Usage: 0.00 MB\n",
            "Optimized Dtypes:\n",
            "A        int8\n",
            "B     float32\n",
            "C    category\n",
            "D         str\n",
            "dtype: object\n",
            "Optimized Memory Usage: 0.00 MB\n"
          ]
        }
      ],
      "source": [
        "# 1. Optimizing Integer and Float Types\n",
        "\n",
        "def downcast_numeric(df):\n",
        "    for col in df.select_dtypes(include=['int64', 'float64']).columns:\n",
        "        # Check limits and downcast to the smallest fit\n",
        "        if 'int' in str(df[col].dtype):\n",
        "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
        "        elif 'float' in str(df[col].dtype):\n",
        "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
        "    return df\n",
        "\n",
        "# Create a sample DataFrame for demonstration\n",
        "import pandas as pd\n",
        "data = {\n",
        "    'A': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    'B': [1.1, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7, 8.8, 9.9, 10.1],\n",
        "    'C': ['apple', 'banana', 'apple', 'orange', 'banana', 'apple', 'grape', 'orange', 'banana', 'apple'],\n",
        "    'D': ['long string data', 'more long string data', 'long string data', 'unique string', 'more long string data', 'long string data', 'some other text', 'unique string', 'more long string data', 'long string data']\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(\"Original Dtypes:\")\n",
        "print(df.dtypes)\n",
        "print(f\"Original Memory Usage: {df.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
        "\n",
        "# 2. Optimizing String Types (Critical for low-cardinality data)\n",
        "\n",
        "# Column 'C' has only 5 unique values (low cardinality)\n",
        "df['C'] = df['C'].astype('category')\n",
        "\n",
        "df_optimized = downcast_numeric(df.copy())\n",
        "\n",
        "# Re-check memory usage\n",
        "optimized_mem = df_optimized.memory_usage(deep=True).sum() / (1024**2)\n",
        "\n",
        "print('Optimized Dtypes:')\n",
        "print(df_optimized.dtypes)\n",
        "print(f\"Optimized Memory Usage: {optimized_mem:.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrZiN1OXmZqt",
        "outputId": "a9589f92-b70c-4230-bdce-e53c417f0bce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pipeline training successful.\n",
            "\n",
            "Predictions on new data: [0 1]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. Sample Data (Simulating raw input)\n",
        "data = {\n",
        "    'Age': [30, 45, np.nan, 22, 60],\n",
        "    'Income': [50000, 120000, 80000, 30000, 150000],\n",
        "    'City': ['NYC', 'London', 'Paris', 'NYC', 'London'],\n",
        "    'Target': [0, 1, 0, 1, 1]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "X = df.drop('Target', axis=1)\n",
        "y = df['Target']\n",
        "\n",
        "# --- Step 1: Define Column Groups ---\n",
        "\n",
        "numerical_features = ['Age', 'Income']\n",
        "categorical_features = ['City']\n",
        "\n",
        "# --- Step 2: Define Sub-Pipelines ---\n",
        "\n",
        "# Pipeline for Numerical Data (Impute missing, then scale)\n",
        "numerical_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Pipeline for Categorical Data (Handle missing, then OHE)\n",
        "categorical_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "\n",
        "])\n",
        "\n",
        "# --- Step 3: Combine Pipelines using ColumnTransformer ---\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_pipeline, numerical_features),\n",
        "        ('cat', categorical_pipeline, categorical_features)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# --- Step 4: Final Model Pipeline ---\n",
        "\n",
        "# The final pipeline integrates preprocessing and the model\n",
        "full_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(solver='liblinear'))\n",
        "])\n",
        "\n",
        "# --- Step 5: Training ---\n",
        "\n",
        "# The entire cleaning/scaling/training process is run in one line\n",
        "full_pipeline.fit(X, y)\n",
        "\n",
        "print(\"Pipeline training successful.\")\n",
        "\n",
        "# --- Step 6: Prediction on New Data ---\n",
        "\n",
        "new_data = pd.DataFrame({\n",
        "    'Age': [40, np.nan],\n",
        "    'Income': [60000, 95000],\n",
        "    'City': ['Paris', 'Berlin']\n",
        "})\n",
        "\n",
        "# The exact scaling and imputation rules learned from the training data are applied\n",
        "predictions = full_pipeline.predict(new_data)\n",
        "\n",
        "print(f\"\\nPredictions on new data: {predictions}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

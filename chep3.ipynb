{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8XgqYfiVNGW",
        "outputId": "53d29bbd-f515-4418-c70e-1c9e036de3f0"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31merror: failed-wheel-build-for-install\n",
            "\u001b[1;31m\n",
            "\u001b[1;31m× Failed to build installable wheels for some pyproject.toml based projects\n",
            "\u001b[1;31m╰─> pyzmq. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Example DataFrames\n",
        "customers = pd.DataFrame({\n",
        "    'customer_id': [1, 2, 3, 4],\n",
        "    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
        "    'region': ['East', 'West', 'East', 'South']\n",
        "})\n",
        "\n",
        "orders = pd.DataFrame({\n",
        "    'order_id': [101, 102, 103, 104, 105],\n",
        "    'customer_id': [1, 2, 2, 5, 3],\n",
        "    'amount': [50.0, 120.0, 80.0, 30.0, 90.0]\n",
        "})\n",
        "\n",
        "\n",
        "merged_df = pd.merge(customers, orders,\n",
        "                     on='customer_id',\n",
        "                     how='outer',\n",
        "                     indicator=True) \n",
        "\n",
        "print(merged_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7zVHK9VVO4F",
        "outputId": "ef3c0024-6f78-49cf-9ff0-f0c4038c8eeb"
      },
      "outputs": [],
      "source": [
        "from os.path import join\n",
        "import pandas as pd\n",
        "\n",
        "customers = pd.DataFrame({\n",
        "    \"customer_id\": [1, 2, 3, 4],\n",
        "    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\n",
        "    \"region\": [\"East\", \"West\", \"East\", \"South\"]\n",
        "})\n",
        "\n",
        "orders = pd.DataFrame({\n",
        "    \"order_id\": [101, 102, 103, 104, 105],\n",
        "    \"customer_id\": [1, 2, 2, 5, 3],\n",
        "    \"amount\": [50.0, 120.0, 80.0, 30.0, 90.0]\n",
        "})\n",
        "\n",
        "join_df = customers.set_index(\"customer_id\").join(\n",
        "    orders.set_index(\"customer_id\"),\n",
        "    how=\"right\"\n",
        ").reset_index()\n",
        "\n",
        "print(join_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_c9mdGkW-YX",
        "outputId": "4413efb1-45a9-4eeb-a3ed-6e4aa46d0698"
      },
      "outputs": [],
      "source": [
        "# Data prepared for pivoting: Sales by Product and Month\n",
        "data = {\n",
        "    'Month': [1, 1, 2, 2, 3, 3],\n",
        "    'Product': ['A', 'B', 'A', 'B', 'A', 'B'],\n",
        "    'Sales': [100, 50, 110, 60, 150, 70]\n",
        "}\n",
        "df_sales = pd.DataFrame(data)\n",
        "\n",
        "# Pivot: Turn products into columns, aggregated by month\n",
        "pivoted_df = df_sales.pivot_table(\n",
        "    index='Month',\n",
        "    columns='Product',\n",
        "    values='Sales',\n",
        "    aggfunc='sum'\n",
        ")\n",
        "\n",
        "print(pivoted_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ide_ECqtYe0w",
        "outputId": "38e1b31d-a0e1-4757-f06a-aecb48e6205b"
      },
      "outputs": [],
      "source": [
        "# Melting the pivoted_df back into a long format\n",
        "melted_df = pd.melt(pivoted_df.reset_index(),\n",
        "                    id_vars=['Month'],\n",
        "                    value_vars=['A', 'B'],\n",
        "                    var_name='Product',\n",
        "                    value_name='Sales')\n",
        "\n",
        "print(melted_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3gD2xOaYpOa",
        "outputId": "5f22abc3-58d0-4147-8534-115d8b67e543"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([[10, 1], [20, 5], [30, 10]])\n",
        "\n",
        "# 1. Normalization (MinMaxScaler)\n",
        "scaler_norm = MinMaxScaler()\n",
        "data_normalized = scaler_norm.fit_transform(data)\n",
        "print('Normalized (Min-Max):')\n",
        "print(data_normalized.round(2))\n",
        "\n",
        "# 2. Standardization (StandardScaler)\n",
        "scaler_std = StandardScaler()\n",
        "data_standardized = scaler_std.fit_transform(data)\n",
        "print('\\nStandardized (Z-Score):')\n",
        "print(data_standardized.round(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irqWFN22mZRh",
        "outputId": "8969c131-794f-4401-94ed-4b33f941c43f"
      },
      "outputs": [],
      "source": [
        "# 1. Optimizing Integer and Float Types\n",
        "\n",
        "def downcast_numeric(df):\n",
        "    for col in df.select_dtypes(include=['int64', 'float64']).columns:\n",
        "        # Check limits and downcast to the smallest fit\n",
        "        if 'int' in str(df[col].dtype):\n",
        "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
        "        elif 'float' in str(df[col].dtype):\n",
        "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
        "    return df\n",
        "\n",
        "# Create a sample DataFrame for demonstration\n",
        "import pandas as pd\n",
        "data = {\n",
        "    'A': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    'B': [1.1, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7, 8.8, 9.9, 10.1],\n",
        "    'C': ['apple', 'banana', 'apple', 'orange', 'banana', 'apple', 'grape', 'orange', 'banana', 'apple'],\n",
        "    'D': ['long string data', 'more long string data', 'long string data', 'unique string', 'more long string data', 'long string data', 'some other text', 'unique string', 'more long string data', 'long string data']\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(\"Original Dtypes:\")\n",
        "print(df.dtypes)\n",
        "print(f\"Original Memory Usage: {df.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
        "\n",
        "# 2. Optimizing String Types (Critical for low-cardinality data)\n",
        "\n",
        "# Column 'C' has only 5 unique values (low cardinality)\n",
        "df['C'] = df['C'].astype('category')\n",
        "\n",
        "df_optimized = downcast_numeric(df.copy())\n",
        "\n",
        "# Re-check memory usage\n",
        "optimized_mem = df_optimized.memory_usage(deep=True).sum() / (1024**2)\n",
        "\n",
        "print('Optimized Dtypes:')\n",
        "print(df_optimized.dtypes)\n",
        "print(f\"Optimized Memory Usage: {optimized_mem:.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrZiN1OXmZqt",
        "outputId": "a9589f92-b70c-4230-bdce-e53c417f0bce"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. Sample Data (Simulating raw input)\n",
        "data = {\n",
        "    'Age': [30, 45, np.nan, 22, 60],\n",
        "    'Income': [50000, 120000, 80000, 30000, 150000],\n",
        "    'City': ['NYC', 'London', 'Paris', 'NYC', 'London'],\n",
        "    'Target': [0, 1, 0, 1, 1]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "X = df.drop('Target', axis=1)\n",
        "y = df['Target']\n",
        "\n",
        "# --- Step 1: Define Column Groups ---\n",
        "\n",
        "numerical_features = ['Age', 'Income']\n",
        "categorical_features = ['City']\n",
        "\n",
        "# --- Step 2: Define Sub-Pipelines ---\n",
        "\n",
        "# Pipeline for Numerical Data (Impute missing, then scale)\n",
        "numerical_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Pipeline for Categorical Data (Handle missing, then OHE)\n",
        "categorical_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "\n",
        "])\n",
        "\n",
        "# --- Step 3: Combine Pipelines using ColumnTransformer ---\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_pipeline, numerical_features),\n",
        "        ('cat', categorical_pipeline, categorical_features)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# --- Step 4: Final Model Pipeline ---\n",
        "\n",
        "# The final pipeline integrates preprocessing and the model\n",
        "full_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(solver='liblinear'))\n",
        "])\n",
        "\n",
        "# --- Step 5: Training ---\n",
        "\n",
        "# The entire cleaning/scaling/training process is run in one line\n",
        "full_pipeline.fit(X, y)\n",
        "\n",
        "print(\"Pipeline training successful.\")\n",
        "\n",
        "# --- Step 6: Prediction on New Data ---\n",
        "\n",
        "new_data = pd.DataFrame({\n",
        "    'Age': [40, np.nan],\n",
        "    'Income': [60000, 95000],\n",
        "    'City': ['Paris', 'Berlin']\n",
        "})\n",
        "\n",
        "# The exact scaling and imputation rules learned from the training data are applied\n",
        "predictions = full_pipeline.predict(new_data)\n",
        "\n",
        "print(f\"\\nPredictions on new data: {predictions}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Merging DataFrames\n",
        "\n",
        "This cell demonstrates how to combine two different data sources (Customers and Orders) using the `pd.merge()` function. We use an **Outer Join** to ensure no data is lost and an **Indicator** to see which table provided the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 3: Data Manipulation & Preprocessing\n",
        "\n",
        "This notebook focuses on advanced data cleaning, reshaping, and preparing data for machine learning using Pandas and Scikit-learn.\n",
        "\n",
        "| Cell # | Purpose | Key Details |\n",
        "| :--- | :--- | :--- |\n",
        "| **1** | **Merging DataFrames** | Uses `pd.merge()` to perform an Outer Join and track sources with an indicator. |\n",
        "| **2** | **Joining Data** | Demonstrates index-based joining using `.join()`. |\n",
        "| **3** | **Pivoting Data** | Transforms 'long' format to 'wide' format using `pivot_table()`. |\n",
        "| **4** | **Melting Data** | Reverses pivoting using `pd.melt()` for data normalization. |\n",
        "| **5** | **Missing Data Analysis** | Calculates counts and percentages of null values. |\n",
        "| **6** | **Basic Imputation** | Demonstrates Median and Forward Fill strategies. |\n",
        "| **7** | **Advanced Imputation** | Uses Scikit-learn's `SimpleImputer` for mean-based filling. |\n",
        "| **8** | **One-Hot Encoding** | Converts categorical names into binary columns. |\n",
        "| **9** | **Ordinal Encoding** | Maps hierarchical categories to specific integers. |\n",
        "| **10** | **Feature Scaling** | Demonstrates Normalization and Standardization. |\n",
        "| **11** | **Memory Optimization** | Downcasts numeric types to save RAM. |\n",
        "| **12** | **ML Pipelines** | Automates the entire preprocessing workflow. |\n",
        "| **13** | **Sales Data Loading** | Imports CSV data and calculates total Revenue per order. |\n",
        "| **14** | **Pivot Table Reporting** | Summarizes sales by category and city for better insight. |\n",
        "| **15** | **Data Reshaping (Melting)** | Converts wide summarized data back to long format. |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8XgqYfiVNGW",
        "outputId": "53d29bbd-f515-4418-c70e-1c9e036de3f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   customer_id     name region  order_id  amount      _merge\n",
            "0            1    Alice   East     101.0    50.0        both\n",
            "1            2      Bob   West     102.0   120.0        both\n",
            "2            2      Bob   West     103.0    80.0        both\n",
            "3            3  Charlie   East     105.0    90.0        both\n",
            "4            4    David  South       NaN     NaN   left_only\n",
            "5            5      NaN    NaN     104.0    30.0  right_only\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Example DataFrames\n",
        "customers = pd.DataFrame({\n",
        "    'customer_id': [1, 2, 3, 4],\n",
        "    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
        "    'region': ['East', 'West', 'East', 'South']\n",
        "})\n",
        "\n",
        "orders = pd.DataFrame({\n",
        "    'order_id': [101, 102, 103, 104, 105],\n",
        "    'customer_id': [1, 2, 2, 5, 3],\n",
        "    'amount': [50.0, 120.0, 80.0, 30.0, 90.0]\n",
        "})\n",
        "\n",
        "\n",
        "merged_df = pd.merge(customers, orders,\n",
        "                     on='customer_id',\n",
        "                     how='outer',\n",
        "                     indicator=True) \n",
        "\n",
        "print(merged_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Joining Data (Index-based)\n",
        "\n",
        "Unlike merging on a column, `.join()` is used to combine DataFrames based on their indexes. This is often faster and cleaner for primary-key associations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N7zVHK9VVO4F",
        "outputId": "ef3c0024-6f78-49cf-9ff0-f0c4038c8eeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   customer_id     name region  order_id  amount\n",
            "0            1    Alice   East       101    50.0\n",
            "1            2      Bob   West       102   120.0\n",
            "2            2      Bob   West       103    80.0\n",
            "3            5      NaN    NaN       104    30.0\n",
            "4            3  Charlie   East       105    90.0\n"
          ]
        }
      ],
      "source": [
        "from os.path import join\n",
        "import pandas as pd\n",
        "\n",
        "customers = pd.DataFrame({\n",
        "    \"customer_id\": [1, 2, 3, 4],\n",
        "    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\"],\n",
        "    \"region\": [\"East\", \"West\", \"East\", \"South\"]\n",
        "})\n",
        "\n",
        "orders = pd.DataFrame({\n",
        "    \"order_id\": [101, 102, 103, 104, 105],\n",
        "    \"customer_id\": [1, 2, 2, 5, 3],\n",
        "    \"amount\": [50.0, 120.0, 80.0, 30.0, 90.0]\n",
        "})\n",
        "\n",
        "join_df = customers.set_index(\"customer_id\").join(\n",
        "    orders.set_index(\"customer_id\"),\n",
        "    how=\"right\"\n",
        ").reset_index()\n",
        "\n",
        "print(join_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Pivoting Data\n",
        "\n",
        "Pivoting allows us to reshape data by turning unique values from one column into multiple new columns. Here, we turn 'Product' rows into columns to see monthly sales at a glance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Product    A   B\n",
            "Month           \n",
            "1        100  50\n",
            "2        110  60\n",
            "3        150  70\n"
          ]
        }
      ],
      "source": [
        "# Data prepared for pivoting: Sales by Product and Month\n",
        "data = {\n",
        "    'Month': [1, 1, 2, 2, 3, 3],\n",
        "    'Product': ['A', 'B', 'A', 'B', 'A', 'B'],\n",
        "    'Sales': [100, 50, 110, 60, 150, 70]\n",
        "}\n",
        "df_sales = pd.DataFrame(data)\n",
        "\n",
        "# Pivot: Turn products into columns, aggregated by month\n",
        "pivoted_df = df_sales.pivot_table(\n",
        "    index='Month',\n",
        "    columns='Product',\n",
        "    values='Sales',\n",
        "    aggfunc='sum'\n",
        ")\n",
        "\n",
        "print(pivoted_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Melting Data\n",
        "\n",
        "Melting is the inverse of pivoting. It unpivots a DataFrame from 'wide' format back to a 'long' format, which is often required for statistical modeling or plotting with libraries like Seaborn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Month Product  Sales\n",
            "0      1       A    100\n",
            "1      2       A    110\n",
            "2      3       A    150\n",
            "3      1       B     50\n",
            "4      2       B     60\n",
            "5      3       B     70\n"
          ]
        }
      ],
      "source": [
        "# Melting the pivoted_df back into a long format\n",
        "melted_df = pd.melt(pivoted_df.reset_index(),\n",
        "                    id_vars=['Month'],\n",
        "                    value_vars=['A', 'B'],\n",
        "                    var_name='Product',\n",
        "                    value_name='Sales')\n",
        "\n",
        "print(melted_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Identifying Missing Data\n",
        "\n",
        "Before cleaning data, we must identify where information is missing. This cell calculates the sum and percentage of `NaN` values per column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing Counts:\n",
            " A    1\n",
            "B    1\n",
            "C    1\n",
            "dtype: int64\n",
            "\n",
            "Missing Percentage:\n",
            " A    25.0\n",
            "B    25.0\n",
            "C    25.0\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = {'A': [1, 2, np.nan, 4], \n",
        "        'B': [5, np.nan, 7, 8], \n",
        "        'C': [9, 10, 11, np.nan]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# 1. Total count of missing values per column\n",
        "print('Missing Counts:\\n', df.isnull().sum())\n",
        "\n",
        "# 2. Percentage of missing values\n",
        "print('\\nMissing Percentage:\\n', (df.isnull().sum() / len(df)) * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Basic Imputation\n",
        "\n",
        "We handle missing data using simple Pandas methods. Here we use the **Median** to fill missing numbers in column 'A' and **Forward Fill** for column 'C'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     A    B     C  A_imputed_median  C_imputed_ffill\n",
            "0  1.0  5.0   9.0               1.0              9.0\n",
            "1  2.0  NaN  10.0               2.0             10.0\n",
            "2  NaN  7.0  11.0               2.0             11.0\n",
            "3  4.0  8.0   NaN               4.0             11.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Impute Column A (Numeric) with the Median\n",
        "median_A = df['A'].median()\n",
        "df['A_imputed_median'] = df['A'].fillna(median_A)\n",
        "\n",
        "# 2. Impute Column C (Time Series) using Forward Fill (FFill)\n",
        "df['C_imputed_ffill'] = df['C'].ffill()\n",
        "\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. SimpleImputer (Scikit-learn)\n",
        "\n",
        "Scikit-learn's `SimpleImputer` provides a standardized way to fill missing values, making it easier to integrate into automated machine learning pipelines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "DataFrame after sklearn mean imputation:\n",
            "          A         B     C  A_imputed_median  C_imputed_ffill\n",
            "0  1.000000  5.000000   9.0               1.0              9.0\n",
            "1  2.000000  6.666667  10.0               2.0             10.0\n",
            "2  2.333333  7.000000  11.0               2.0             11.0\n",
            "3  4.000000  8.000000   NaN               4.0             11.0\n"
          ]
        }
      ],
      "source": [
        "# Using Scikit-learn to handle multiple columns with different strategies\n",
        "imputer_mean = SimpleImputer(strategy='mean')\n",
        "\n",
        "# Fit and transform (A and B are imputed with their respective means)\n",
        "df[['A', 'B']] = imputer_mean.fit_transform(df[['A', 'B']])\n",
        "\n",
        "print('\\nDataFrame after sklearn mean imputation:')\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. One-Hot Encoding\n",
        "\n",
        "Machine learning models cannot work with text directly. One-Hot Encoding converts categorical names (like Cities) into new columns with 0s and 1s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "One-Hot Encoded Data:\n",
            "   Price  is_London  is_NYC  is_Paris\n",
            "0    100      False    True     False\n",
            "1    200       True   False     False\n",
            "2    300      False   False      True\n",
            "3    150      False    True     False\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    'City': ['NYC', 'London', 'Paris', 'NYC'],\n",
        "    'Price': [100, 200, 300, 150]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Perform OHE\n",
        "df_ohe = pd.get_dummies(df, columns=['City'], prefix='is')\n",
        "\n",
        "print('One-Hot Encoded Data:')\n",
        "print(df_ohe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Ordinal Encoding\n",
        "\n",
        "When data has a clear order (e.g., Small < Medium < Large), we use Ordinal Encoding to convert categories into a meaningful sequence of numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ordinal Encoded Data:\n",
            "     Size  Value  Size_Encoded\n",
            "0   Small     10           0.0\n",
            "1  Medium     20           1.0\n",
            "2   Large     30           2.0\n",
            "3   Small     15           0.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import numpy as np\n",
        "\n",
        "data = {\n",
        "    'Size': ['Small', 'Medium', 'Large', 'Small'],\n",
        "    'Value': [10, 20, 30, 15]\n",
        "}\n",
        "df_ord = pd.DataFrame(data)\n",
        "\n",
        "# Define the explicit order\n",
        "size_order = ['Small', 'Medium', 'Large']\n",
        "\n",
        "encoder = OrdinalEncoder(categories=[size_order])\n",
        "\n",
        "# Fit and transform the 'Size' column\n",
        "df_ord['Size_Encoded'] = encoder.fit_transform(df_ord[['Size']])\n",
        "\n",
        "print('\\nOrdinal Encoded Data:')\n",
        "print(df_ord)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Feature Scaling\n",
        "\n",
        "This cell compares **Normalization** (scaling values to a 0-1 range) and **Standardization** (scaling to a mean of 0 and standard deviation of 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3gD2xOaYpOa",
        "outputId": "5f22abc3-58d0-4147-8534-115d8b67e543"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normalized (Min-Max):\n",
            "[[0.   0.  ]\n",
            " [0.5  0.44]\n",
            " [1.   1.  ]]\n",
            "\n",
            "Standardized (Z-Score):\n",
            "[[-1.22 -1.18]\n",
            " [ 0.   -0.09]\n",
            " [ 1.22  1.27]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([[10, 1], [20, 5], [30, 10]])\n",
        "\n",
        "# 1. Normalization (MinMaxScaler)\n",
        "scaler_norm = MinMaxScaler()\n",
        "data_normalized = scaler_norm.fit_transform(data)\n",
        "print('Normalized (Min-Max):')\n",
        "print(data_normalized.round(2))\n",
        "\n",
        "# 2. Standardization (StandardScaler)\n",
        "scaler_std = StandardScaler()\n",
        "data_standardized = scaler_std.fit_transform(data)\n",
        "print('\\nStandardized (Z-Score):')\n",
        "print(data_standardized.round(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Memory Optimization\n",
        "\n",
        "Handling big data requires efficiency. This custom function reduces memory usage by converting large numeric types to smaller byte sizes and using the `category` type for repetitive strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irqWFN22mZRh",
        "outputId": "8969c131-794f-4401-94ed-4b33f941c43f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Dtypes:\n",
            "A      int64\n",
            "B    float64\n",
            "C        str\n",
            "D        str\n",
            "dtype: object\n",
            "Original Memory Usage: 0.00 MB\n",
            "Optimized Dtypes:\n",
            "A        int8\n",
            "B     float32\n",
            "C    category\n",
            "D         str\n",
            "dtype: object\n",
            "Optimized Memory Usage: 0.00 MB\n"
          ]
        }
      ],
      "source": [
        "# 1. Optimizing Integer and Float Types\n",
        "\n",
        "def downcast_numeric(df):\n",
        "    for col in df.select_dtypes(include=['int64', 'float64']).columns:\n",
        "        # Check limits and downcast to the smallest fit\n",
        "        if 'int' in str(df[col].dtype):\n",
        "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
        "        elif 'float' in str(df[col].dtype):\n",
        "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
        "    return df\n",
        "\n",
        "# Create a sample DataFrame for demonstration\n",
        "import pandas as pd\n",
        "data = {\n",
        "    'A': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "    'B': [1.1, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7, 8.8, 9.9, 10.1],\n",
        "    'C': ['apple', 'banana', 'apple', 'orange', 'banana', 'apple', 'grape', 'orange', 'banana', 'apple'],\n",
        "    'D': ['long string data', 'more long string data', 'long string data', 'unique string', 'more long string data', 'long string data', 'some other text', 'unique string', 'more long string data', 'long string data']\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(\"Original Dtypes:\")\n",
        "print(df.dtypes)\n",
        "print(f\"Original Memory Usage: {df.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
        "\n",
        "# 2. Optimizing String Types (Critical for low-cardinality data)\n",
        "\n",
        "# Column 'C' has only 5 unique values (low cardinality)\n",
        "df['C'] = df['C'].astype('category')\n",
        "\n",
        "df_optimized = downcast_numeric(df.copy())\n",
        "\n",
        "# Re-check memory usage\n",
        "optimized_mem = df_optimized.memory_usage(deep=True).sum() / (1024**2)\n",
        "\n",
        "print('Optimized Dtypes:')\n",
        "print(df_optimized.dtypes)\n",
        "print(f\"Optimized Memory Usage: {optimized_mem:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. End-to-End ML Pipelines\n",
        "\n",
        "This complex pipeline automatically handles missing data, scales numerical features, encodes categories, and trains a Logistic Regression modelâ€”all in one unified workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrZiN1OXmZqt",
        "outputId": "a9589f92-b70c-4230-bdce-e53c417f0bce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pipeline training successful.\n",
            "\n",
            "Predictions on new data: [0 1]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. Sample Data (Simulating raw input)\n",
        "data = {\n",
        "    'Age': [30, 45, np.nan, 22, 60],\n",
        "    'Income': [50000, 120000, 80000, 30000, 150000],\n",
        "    'City': ['NYC', 'London', 'Paris', 'NYC', 'London'],\n",
        "    'Target': [0, 1, 0, 1, 1]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "X = df.drop('Target', axis=1)\n",
        "y = df['Target']\n",
        "\n",
        "# --- Step 1: Define Column Groups ---\n",
        "\n",
        "numerical_features = ['Age', 'Income']\n",
        "categorical_features = ['City']\n",
        "\n",
        "# --- Step 2: Define Sub-Pipelines ---\n",
        "\n",
        "# Pipeline for Numerical Data (Impute missing, then scale)\n",
        "numerical_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Pipeline for Categorical Data (Handle missing, then OHE)\n",
        "categorical_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "\n",
        "])\n",
        "\n",
        "# --- Step 3: Combine Pipelines using ColumnTransformer ---\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_pipeline, numerical_features),\n",
        "        ('cat', categorical_pipeline, categorical_features)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# --- Step 4: Final Model Pipeline ---\n",
        "\n",
        "# The final pipeline integrates preprocessing and the model\n",
        "full_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(solver='liblinear'))\n",
        "])\n",
        "\n",
        "# --- Step 5: Training ---\n",
        "\n",
        "# The entire cleaning/scaling/training process is run in one line\n",
        "full_pipeline.fit(X, y)\n",
        "\n",
        "print(\"Pipeline training successful.\")\n",
        "\n",
        "# --- Step 6: Prediction on New Data ---\n",
        "\n",
        "new_data = pd.DataFrame({\n",
        "    'Age': [40, np.nan],\n",
        "    'Income': [60000, 95000],\n",
        "    'City': ['Paris', 'Berlin']\n",
        "})\n",
        "\n",
        "# The exact scaling and imputation rules learned from the training data are applied\n",
        "predictions = full_pipeline.predict(new_data)\n",
        "\n",
        "print(f\"\\nPredictions on new data: {predictions}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Sales Data Analysis: Loading & Exploration\n",
        "\n",
        "This section identifies how to load raw transactional data from a CSV file and perform initial inspection to understand the product categories and regional distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   order_id customer_id customer_name   product     category  price  quantity  \\\n",
            "0       101        C001         Alice    Laptop  Electronics    800         1   \n",
            "1       102        C002           Bob     Phone  Electronics    500         2   \n",
            "2       103        C001         Alice     Mouse  Accessories     20         3   \n",
            "3       104        C003       Charlie  Keyboard  Accessories     50         1   \n",
            "4       105        C002           Bob    Laptop  Electronics    800         1   \n",
            "\n",
            "        city  order_date  \n",
            "0  Kathmandu  2024-01-05  \n",
            "1   Lalitpur  2024-01-06  \n",
            "2  Kathmandu  2024-01-10  \n",
            "3  Bhaktapur  2024-02-01  \n",
            "4   Lalitpur  2024-02-05  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"sales_data.csv\")\n",
        "\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Feature Engineering: Calculating Revenue\n",
        "\n",
        "Derived features are often necessary for analysis. Here, we calculate the total Revenue for each order by multiplying the unit price with the quantity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   order_id customer_id customer_name   product     category  price  quantity  \\\n",
            "0       101        C001         Alice    Laptop  Electronics    800         1   \n",
            "1       102        C002           Bob     Phone  Electronics    500         2   \n",
            "2       103        C001         Alice     Mouse  Accessories     20         3   \n",
            "3       104        C003       Charlie  Keyboard  Accessories     50         1   \n",
            "4       105        C002           Bob    Laptop  Electronics    800         1   \n",
            "\n",
            "        city  order_date  Revenue  \n",
            "0  Kathmandu  2024-01-05      800  \n",
            "1   Lalitpur  2024-01-06     1000  \n",
            "2  Kathmandu  2024-01-10       60  \n",
            "3  Bhaktapur  2024-02-01       50  \n",
            "4   Lalitpur  2024-02-05      800  \n"
          ]
        }
      ],
      "source": [
        "df[\"Revenue\"] = df[\"price\"] * df[\"quantity\"]\n",
        "\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "category\n",
            "Accessories     200\n",
            "Electronics    3100\n",
            "Name: Revenue, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "cateegory_summary = df.groupby('category')[\"Revenue\"].sum()\n",
        "\n",
        "print(cateegory_summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Pivot Table Reporting\n",
        "\n",
        "This cell creates a cross-tabulation of total Revenue indexed by Product Category and columned by City. This multi-dimensional view quickly highlights which regions dominate specific markets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "city         Bhaktapur  Kathmandu  Lalitpur\n",
            "category                                   \n",
            "Accessories         90        110         0\n",
            "Electronics          0       1300      1800\n"
          ]
        }
      ],
      "source": [
        "pivoted_df = df.pivot_table(\n",
        "    values='Revenue',\n",
        "    index='category',\n",
        "    columns='city',    \n",
        "    aggfunc='sum',\n",
        "    fill_value=0\n",
        ")\n",
        "print(pivoted_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16. Data Reshaping: Melting the Pivot Table\n",
        "\n",
        "The `.melt()` method is the inverse of pivoting. It converts the wide-format summarized table back into a long-format DataFrame, which is essential for certain visualization and database operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      category       city  Revenue\n",
            "0  Accessories  Bhaktapur       90\n",
            "1  Electronics  Bhaktapur        0\n",
            "2  Accessories  Kathmandu      110\n",
            "3  Electronics  Kathmandu     1300\n",
            "4  Accessories   Lalitpur        0\n"
          ]
        }
      ],
      "source": [
        "melted_df = pivoted_df.reset_index().melt(\n",
        "    id_vars='category',\n",
        "    var_name='city',\n",
        "    value_name='Revenue'\n",
        ")\n",
        "print(melted_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   order_id   product  quantity     category  price customer_id  order_date  \\\n",
            "0       101    Laptop         1  Electronics    800        C001  2024-01-05   \n",
            "1       102     Phone         2  Electronics    500        C002  2024-01-06   \n",
            "2       103     Mouse         3  Accessories     20        C001  2024-01-10   \n",
            "3       104  Keyboard         1  Accessories     50        C003  2024-02-01   \n",
            "4       105    Laptop         1  Electronics    800        C002  2024-02-05   \n",
            "\n",
            "  customer_name       city  \n",
            "0         Alice  Kathmandu  \n",
            "1           Bob   Lalitpur  \n",
            "2         Alice  Kathmandu  \n",
            "3       Charlie  Bhaktapur  \n",
            "4           Bob   Lalitpur  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "customers = df[[\"customer_id\", \"customer_name\", \"city\"]].drop_duplicates()\n",
        "products = df[[\"product\", \"category\", \"price\"]].drop_duplicates()\n",
        "orders = df[[\"order_id\", \"customer_id\", \"order_date\"]].drop_duplicates()\n",
        "order_details = df[[\"order_id\", \"product\", \"quantity\"]].drop_duplicates()\n",
        "\n",
        "merged_df = order_details.merge(products, on=\"product\", how=\"left\") \\\n",
        "                         .merge(orders, on=\"order_id\", how=\"left\") \\\n",
        "                         .merge(customers, on=\"customer_id\", how=\"left\")\n",
        "\n",
        "print(merged_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Log Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Values  Log_Values\n",
            "0       1    0.000000\n",
            "1       2    0.301030\n",
            "2       3    0.477121\n",
            "3       4    0.602060\n",
            "4       5    0.698970\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = {\n",
        "    'Values': [1, 2, 3, 4, 5]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "df['Log_Values'] = np.log10(df['Values'])\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Means (Col A, Col B): [200.   2.]\n",
            "Scaled Means (should be close to 0): [0. 0.]\n",
            "Scaled Data:\n",
            " [[-1.22474487 -1.22474487]\n",
            " [ 0.          0.        ]\n",
            " [ 1.22474487  1.22474487]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([\n",
        "    [100, 1],\n",
        "    [200, 2],\n",
        "    [300, 3]\n",
        "])\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\"Original Means (Col A, Col B):\", np.mean(X, axis=0))\n",
        "print(\"Scaled Means (should be close to 0):\", np.mean(X_scaled, axis=0).round(2))\n",
        "print(\"Scaled Data:\\n\", X_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

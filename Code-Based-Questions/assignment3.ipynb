{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-sec3",
   "metadata": {},
   "source": [
    "# Code-Based Questions: Assignment 3\n",
    "## 2.1.1 Advanced Data Wrangling with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q41",
   "metadata": {},
   "source": [
    "### Q41. Write Pandas code to merge two datasets using inner and outer joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner Join Results:\n",
      "   key val1 val2\n",
      "0    2    B    X\n",
      "1    3    C    Y\n",
      "\n",
      "Outer Join Results:\n",
      "   key val1 val2\n",
      "0    1    A  NaN\n",
      "1    2    B    X\n",
      "2    3    C    Y\n",
      "3    4  NaN    Z\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.DataFrame({'key': [1, 2, 3], 'val1': ['A', 'B', 'C']})\n",
    "df2 = pd.DataFrame({'key': [2, 3, 4], 'val2': ['X', 'Y', 'Z']})\n",
    "\n",
    "inner_merge = pd.merge(df1, df2, on='key', how='inner')\n",
    "outer_merge = pd.merge(df1, df2, on='key', how='outer')\n",
    "\n",
    "print(\"Inner Join Results:\")\n",
    "print(inner_merge)\n",
    "print(\"\\nOuter Join Results:\")\n",
    "print(outer_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q42",
   "metadata": {},
   "source": [
    "### Q42. Given a dataset, create a pivot table showing total sales per category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pivot Table (Total Sales per Category):\n",
      "             Sales\n",
      "Category          \n",
      "Clothing      1100\n",
      "Electronics   2200\n",
      "Home           800\n"
     ]
    }
   ],
   "source": [
    "sales_df = pd.DataFrame({\n",
    "    'Category': ['Electronics', 'Clothing', 'Electronics', 'Home', 'Clothing'],\n",
    "    'Sales': [1000, 500, 1200, 800, 600]\n",
    "})\n",
    "\n",
    "pivot = sales_df.pivot_table(values='Sales', index='Category', aggfunc='sum')\n",
    "print(\"Pivot Table (Total Sales per Category):\")\n",
    "print(pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q43",
   "metadata": {},
   "source": [
    "### Q43. Reshape a DataFrame from wide format to long format using melt()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melted DataFrame (Long Format):\n",
      "   ID  Subject  Score\n",
      "0   1     Math     90\n",
      "1   2     Math     80\n",
      "2   1  Science     85\n",
      "3   2  Science     88\n"
     ]
    }
   ],
   "source": [
    "wide_df = pd.DataFrame({\n",
    "    'ID': [1, 2],\n",
    "    'Math': [90, 80],\n",
    "    'Science': [85, 88]\n",
    "})\n",
    "\n",
    "long_df = wide_df.melt(id_vars=['ID'], var_name='Subject', value_name='Score')\n",
    "print(\"Melted DataFrame (Long Format):\")\n",
    "print(long_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q44",
   "metadata": {},
   "source": [
    "### Q44. Write Python code to detect and handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing counts:\n",
      " A    1\n",
      "B    2\n",
      "dtype: int64\n",
      "\n",
      "Handled DataFrame:\n",
      "          A    B\n",
      "0  1.000000  5.0\n",
      "1  2.000000  0.0\n",
      "2  2.333333  0.0\n",
      "3  4.000000  8.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "df_missing = pd.DataFrame({'A': [1, 2, np.nan, 4], 'B': [5, np.nan, np.nan, 8]})\n",
    "\n",
    "print(\"Missing counts:\\n\", df_missing.isnull().sum())\n",
    "\n",
    "# Handle: Fill A with mean, drop rows from B if too many nulls\n",
    "df_missing['A'] = df_missing['A'].fillna(df_missing['A'].mean())\n",
    "df_cleaned = df_missing.fillna(0)\n",
    "print(\"\\nHandled DataFrame:\")\n",
    "print(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q45",
   "metadata": {},
   "source": [
    "### Q45. Encode categorical variables using one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Encoded cities:\n",
      "   City_Berlin  City_Madrid  City_Paris\n",
      "0         True        False       False\n",
      "1        False        False        True\n",
      "2         True        False       False\n",
      "3        False         True       False\n"
     ]
    }
   ],
   "source": [
    "df_cat = pd.DataFrame({'City': ['Berlin', 'Paris', 'Berlin', 'Madrid']})\n",
    "one_hot = pd.get_dummies(df_cat, columns=['City'])\n",
    "print(\"One-Hot Encoded cities:\")\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q46",
   "metadata": {},
   "source": [
    "### Q46. Convert a column into datetime format and perform time-based indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-based Indexed Data:\n",
      "date_str    2023-01-02\n",
      "value               20\n",
      "Name: 2023-01-02 00:00:00, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_time = pd.DataFrame({\n",
    "    'date_str': ['2023-01-01', '2023-01-02', '2023-01-03'],\n",
    "    'value': [10, 20, 30]\n",
    "})\n",
    "\n",
    "df_time['datetime'] = pd.to_datetime(df_time['date_str'])\n",
    "df_time.set_index('datetime', inplace=True)\n",
    "print(\"Time-based Indexed Data:\")\n",
    "print(df_time.loc['2023-01-02'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q47",
   "metadata": {},
   "source": [
    "### Q47. Apply forward fill and backward fill on time-series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Fill:\n",
      " 0    1.0\n",
      "1    1.0\n",
      "2    1.0\n",
      "3    4.0\n",
      "4    5.0\n",
      "dtype: float64\n",
      "\n",
      "Backward Fill:\n",
      " 0    1.0\n",
      "1    4.0\n",
      "2    4.0\n",
      "3    4.0\n",
      "4    5.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "ts_data = pd.Series([1, np.nan, np.nan, 4, 5])\n",
    "ffill = ts_data.ffill()\n",
    "bfill = ts_data.bfill()\n",
    "\n",
    "print(\"Forward Fill:\\n\", ffill)\n",
    "print(\"\\nBackward Fill:\\n\", bfill)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q48",
   "metadata": {},
   "source": [
    "### Q48. Normalize and standardize numerical features using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized:\n",
      " 0    0.00\n",
      "1    0.25\n",
      "2    0.50\n",
      "3    0.75\n",
      "4    1.00\n",
      "dtype: float64\n",
      "\n",
      "Standardized:\n",
      " 0   -1.264911\n",
      "1   -0.632456\n",
      "2    0.000000\n",
      "3    0.632456\n",
      "4    1.264911\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "nums = pd.Series([10, 20, 30, 40, 50])\n",
    "\n",
    "# Normalization (Min-Max Scaling)\n",
    "normalized = (nums - nums.min()) / (nums.max() - nums.min())\n",
    "\n",
    "# Standardization (Z-score)\n",
    "standardized = (nums - nums.mean()) / nums.std()\n",
    "\n",
    "print(\"Normalized:\\n\", normalized)\n",
    "print(\"\\nStandardized:\\n\", standardized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q49",
   "metadata": {},
   "source": [
    "### Q49. Write code to reduce memory usage of a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Memory: 16132\n",
      "After Memory: 6132\n"
     ]
    }
   ],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'float64':\n",
    "            df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "        if df[col].dtype == 'int64':\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    return df\n",
    "\n",
    "big_df = pd.DataFrame({'a': range(1000), 'b': [1.1]*1000})\n",
    "print(\"Before Memory:\", big_df.memory_usage(deep=True).sum())\n",
    "reduced_df = reduce_mem_usage(big_df)\n",
    "print(\"After Memory:\", reduced_df.memory_usage(deep=True).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q50",
   "metadata": {},
   "source": [
    "### Q50. Process a large CSV file using chunking in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking implementation: pd.read_csv(file, chunksize=N)\n"
     ]
    }
   ],
   "source": [
    "# Demonstrating chunking pattern\n",
    "def process_chunks(filename):\n",
    "    total_rows = 0\n",
    "    # for chunk in pd.read_csv(filename, chunksize=500):\n",
    "    #     total_rows += len(chunk)\n",
    "    # return total_rows\n",
    "    print(\"Chunking implementation: pd.read_csv(file, chunksize=N)\")\n",
    "\n",
    "process_chunks('dummy.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q51",
   "metadata": {},
   "source": [
    "### Q51. Write a reusable data-cleaning function in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard reusable cleaner defined.\n"
     ]
    }
   ],
   "source": [
    "def clean_data(df):\n",
    "    df = df.drop_duplicates()\n",
    "    df = df.fillna(df.mean(numeric_only=True))\n",
    "    # Strip whitespace from strings\n",
    "    for col in df.select_dtypes(include=['object']): \n",
    "        df[col] = df[col].str.strip()\n",
    "    return df\n",
    "\n",
    "print(\"Standard reusable cleaner defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q52",
   "metadata": {},
   "source": [
    "### Q52. Implement a simple data-cleaning pipeline using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame pipeline defined.\n"
     ]
    }
   ],
   "source": [
    "def data_pipeline(df):\n",
    "    return (df\n",
    "            .pipe(lambda x: x.drop_duplicates())\n",
    "            .pipe(lambda x: x.fillna(0))\n",
    "            .pipe(lambda x: x.rename(columns=lambda c: c.lower()))\n",
    "           )\n",
    "\n",
    "print(\"DataFrame pipeline defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q53",
   "metadata": {},
   "source": [
    "### Q53. Write code to ingest data from CSV, database, and API sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion methods defined.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import requests\n",
    "\n",
    "# 1. CSV\n",
    "# csv_data = pd.read_csv('file.csv')\n",
    "\n",
    "# 2. Database\n",
    "conn = sqlite3.connect(':memory:')\n",
    "db_data = pd.read_sql('SELECT * FROM sqlite_master', conn)\n",
    "\n",
    "# 3. API\n",
    "# api_resp = requests.get('https://api.example.com/data')\n",
    "# api_data = pd.DataFrame(api_resp.json())\n",
    "\n",
    "print(\"Ingestion methods defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q54",
   "metadata": {},
   "source": [
    "### Q54. Merge multiple datasets into a unified table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified Table:\n",
      "   ID   A   B   C\n",
      "0   1  10  20  30\n"
     ]
    }
   ],
   "source": [
    "dfs = [\n",
    "    pd.DataFrame({'ID': [1], 'A': [10]}),\n",
    "    pd.DataFrame({'ID': [1], 'B': [20]}),\n",
    "    pd.DataFrame({'ID': [1], 'C': [30]})\n",
    "]\n",
    "\n",
    "unified = dfs[0]\n",
    "for d in dfs[1:]: \n",
    "    unified = pd.merge(unified, d, on='ID')\n",
    "\n",
    "print(\"Unified Table:\")\n",
    "print(unified)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q55",
   "metadata": {},
   "source": [
    "### Q55. Implement RFM (Recency, Frequency, Monetary) analysis using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFM Analysis Table:\n",
      "            Recency  Frequency  Monetary\n",
      "CustomerID                              \n",
      "1                30          2       250\n",
      "2                46          1       200\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "orders = pd.DataFrame({\n",
    "    'CustomerID': [1, 1, 2],\n",
    "    'OrderDate': pd.to_datetime(['2023-10-01', '2023-12-01', '2023-11-15']),\n",
    "    'Amount': [100, 150, 200]\n",
    "})\n",
    "\n",
    "snapshot_date = datetime(2023, 12, 31)\n",
    "rfm = orders.groupby('CustomerID').agg({\n",
    "    'OrderDate': lambda x: (snapshot_date - x.max()).days,\n",
    "    'CustomerID': 'count',\n",
    "    'Amount': 'sum'\n",
    "}).rename(columns={'OrderDate': 'Recency', 'CustomerID': 'Frequency', 'Amount': 'Monetary'})\n",
    "\n",
    "print(\"RFM Analysis Table:\")\n",
    "print(rfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q56",
   "metadata": {},
   "source": [
    "### Q56. Compute Customer Lifetime Value (CLV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLV Computation:\n",
      "             CLV\n",
      "CustomerID      \n",
      "1           3000\n",
      "2           2400\n"
     ]
    }
   ],
   "source": [
    "# Simple CLV: Monthly Revenue * Average Lifespan\n",
    "rfm['CLV'] = rfm['Monetary'] * 12 # Simplified example for 1 year\n",
    "print(\"CLV Computation:\")\n",
    "print(rfm[['CLV']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q57",
   "metadata": {},
   "source": [
    "### Q57. Store processed data into a CSV file or database table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storage logic: df.to_csv() or df.to_sql()\n"
     ]
    }
   ],
   "source": [
    "# rfm.to_csv('processed_rfm.csv')\n",
    "# rfm.to_sql('rfm_analysis', conn, if_exists='replace')\n",
    "print(\"Storage logic: df.to_csv() or df.to_sql()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q58",
   "metadata": {},
   "source": [
    "### Q58. Design a simple end-to-end data pipeline using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full ETL Pipeline Architecture defined.\n"
     ]
    }
   ],
   "source": [
    "def end_to_end_pipeline(source_path, target_path):\n",
    "    # 1. Ingest\n",
    "    # df = pd.read_csv(source_path)\n",
    "    # 2. Process\n",
    "    # df_clean = clean_data(df)\n",
    "    # 3. Store\n",
    "    # df_clean.to_csv(target_path)\n",
    "    print(\"Full ETL Pipeline Architecture defined.\")\n",
    "\n",
    "end_to_end_pipeline('raw.csv', 'clean.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
